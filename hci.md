# Week 1

## Lecture 1.1

- The human doesn't interact with a computer — in actuality, the human interacts with a _task_, through a computer. The goal is to let the user interact as much as possible with the task, 
rather than interacting with the interface/software. The interface should vanish, be seamless.

- e.g., seamless: many video games. Non-seamless: TV + cable remote controls.

- HCI is everywhere

- HCI is a subset of human factors engineering (like industrial design and product design). There are also subdisciplines of HCI: UI design, UX design, interaction design.

- HCI vs. human factors: human factors is interested in designing interactions between products, systems, or devices (both computer and non-computer products, like dimensions of furniture or product). 

- Many products are becoming more and more dependent on computers — consequently, HCI is everywhere, is only getitng more important.

- For many years, HCI had to do with UI design — helping people interact with screens. Consequently, UI design focuses on on-screen interaction, while HCI, broadly, 
applies to any interface. 

- HCI vs. UX design: HCI pertains to *understanding* the interactions between users and computers, whereas as UX design pertains to *dictating* interactions between
users and computers. To design well, you need to *understand* the user. HCI and UX are two symbiotic concepts — we use the results of our designs to conduct research. 

- Human factors engineering is the merging of Engineering (software engineering, in HCI's case) and Psychology. We use our understanding of psychology to inform
the way we design interfaces; we then use those results to better understand the results of our psychology. E.g., experiment re: people organizing incoming 
materials into piles, and designing a UI to mimic this; this UI can help us think about psychology in novel ways.

- Much of HCI is about research: methods research the user, understand their needs, and evaluating the prototypes we create. On the other hand, HCI is also about design:
designing interactions. These two ideas are symbiotic: research informs design, and design informs research. These feeback cycles are an essential part of HCI: using research
to inform designs, and using design to inform research.

- What will we cover? 

Fundamental HCI design principles

Performing user research

Relationship between design and research

HCI applications

## Lecture 1.2

###  Learning goals (what to know)

1. Understanding common principles of HCI
2. Design lifecycle, and particularly, the role of iterations
3. Expanse of the HCI field and its applications in the modern world

### Learning outcome (what to know how to do): Design effective interactions between humans and computers

#### Design

- What's design, exactly?

1. Design- known principles applied to a new problem
1. Design- it's an iterative process where you gather info, prototype, test those designs, and re-evaluate.

We'll jump into both of these in units 2 (principles) and 3 (methods)

#### Effectiveness

Whether something is effective or not depends on the goals at hand. 

- Is the goal usability for the user? 
- Is it research, and deriving insights into human psychology? 
- Is it the goal to change the activity altogether?

#### Between humans and computers

We're interested in designing INTERACTIONS AND TASKS, not the interfaces they use to accomplish them. Take a thermostat: our goal *isn't* to design a thermostat, but
the way *people control temperature in their homes*.


### Learning strategies

We'll learn by example and by reflection. 


 ## Lesson 3

 HCI is now everywhere, because computers are ubiquitous. We'll be discussing HCI in the context of 3 things:

 1. Technology - *emerging* technological capabilities that let us create new user interactions
 2. Ideas - ideas re: the ways that people can interact with interfaces and the world around them
 3. Domains - existing areas that can be significantly disrupted by computer interfaces

###  Technology: VR

This is a wholly new classification of interaction and visualization. Interesting applications:

- Using VR to treat phobias, where people can authentically and realistically confront fears

### Technology: AR

This doesn't replace (like VR) but augments and compliments inputs from the real world. Examples include Google Glass, or Google's translation headphones. Think, additionally,
of the possibilities of AR headsets/phones giving students additional details on paintings custom tailored to students' own interests [THIS COULD BE SUCH A COOL THING!!!]

### Technology: Ubiquitous computing

Deeply related to the internet of things, but today, because microprocessors are v. cheap, computers are EVERYWHERE. Modern HCI is incorporated into every smart device. 
This has led to the rise of wearable tech, like fit bits. 

### Technology: Robotics

while much of the focus on robotics today is on the physical robots, but human-robot interaction is only going to grow. How do we pragmatically equip robots
to interact with humans, and give humans feedback re: robot interaction? Can we create robots that teach things to humans? [POST LINKS TO PUDDING PIECES!!!]

### Technology: Mobile

Yes, screens are small; input methods are imprecise; users are often distracted. Thanks to mobile, we can support experiences from navigation to stargazing. 

### Technology: Context-sensitive computing

Context is an essential part of our communication. To build computers, we must consequently incorporate contet into computing. For example, we use 
our phones differently based on whether we're driving in our cars, or relaxing at home. There's TONS of research to be done here!

### Idea: Gesture-based interaction

We communicate with gestures to a tremendous degree, and this has begun to be incorporated into HCI. Fingers have a huge amount of dexterity, and perhaps
one day we'll be able to detect their muscular movements without needing a keyboard, which would potentially allow us to replace smartphones.

### Idea: Pen- and touch-based interfaces

For the longest time, we directly interacted with the things that we built. Then computers came around, and we needed interfaces. We then returned to 
more direct methods, like touch screens, which allowed interacting with computers with fewer levels of interfaces in between the user and the computer/task. We've 
now moved on to tablet-based interacion to include pens.

### Idea: Information visualization

### Idea: CSCW, computer-supported cooperative work

Distributed teams are an example of this. We can think of designing for two variables: time and place (same + different, time + place). While most often 
this assists people located across dif. times + places, it can also assist those of us who want to collaborate more productively at the same location/time.

### Idea: Social computing

This pertains to how computers affect the ways in which we interact and socialize. Examples of this include emojis, online gaming, Wiki, and dating websites. 

### Domain: Special needs

Computing can help compensate for age, disability, or illness. Another example: how do you communicate data to a blind person (Bruce Walker works on this at GA Tech;
reach out to him?)

### Domain: Education

Interestingly, learning tasks require them to be challenging in order to truly teach students, and HCI can help make certain things more challenging in the 
appropriate way.

### Domain: Healthcare

A central healthcare problem pertains to organizing the vast quantities of information that we generate every day. To make it useful, HCI helps doctors to more easily
explore diagnoses; lets patients see their treatments; or exercise monitoring, or my fitness pal. Virtual reality exercise is already on the way. Therapy is another 
context where this can occur!

### Idea: Security

People hate when security measures get in the way of their tasks; if security isn't usable, it's not useful. HCI can help with this. CAPTCHAs, for example, 
used to have people evaluate images, but now computers can simply detect the mouse movements to see if a human or an algorithm is moving it.

### Idea: Video games

Games are one of the most obvious examples of HCI. In gaming, oftentimes issues with the game are actually HCI issues (task too hard, controls bad, etc.)


## Lesson 4 - Principles of HCI

We focus on users and tasks of HCI; we talk about the role of the interface as a mediating factor; we'll talk about UX more generally.

### Interfaces

Users use these to accomplish tasks — in this course, ones that are computerized. We'll be focused on the relationship between the user and task rather than 
the user and interface, because the key is performing the task. e.g., if you focus on designing the interface for a new thermostat, you'll try to 
place buttons in a nice way. If you're trying to focus on the task, however, you can create new, revolutionary designs, like the Nest thermostat, which focuss on 
regulating temperature.

### 5 tips for identifying a user task

1. Watch REAL Users
2. TALK to them! Ask them what they're thinking, goals, and motives!
3. Start small — start by looking at small, individual interactions
4. Abstract up - working from small observations in #3, try to abstract up to an understanding of which tasks the user actually want to complete.
5. YOU are not the user — you're designing for people who don't have your experience.

### Usefulness and usability - our goals for interfaces

Usefulness - whether or not an interface that allows users to achieve some task. 

Usable - how easy something it is to use (we focus on these with interace).

It's through understanding the task of navigation, for example, that we've offloaded much of the cognitive load from the user (looking at map and figuring out 
where we need to turn at each intersection) to the interface (Google maps, which tells us where we want to turn).

### User roles

The user can play three roles in HCI:

1. The Processor
2. The Predictor
3. The Participant

_The Processor_
A sensory processor: take input in, and spit output out. If we're designing an interface with this in mind, we need to keep human limits in mind: what humans
can sense, what they can store in memory, and what they can physically do in the world. In this case, usability means that the interface is *physically* usable.

Interface must: keep to human limits

How to evaluate: using quantitative experiments.

_The predictor_
We can about the experience, thought process, and knowledge. We want them to predict what will happen in the world as a result of some action they'll take. We 
want them to map input to output. This means we need to get inside their head. 

Interfaces must: fit with human knowledge; help user learn what they don't know, and leverage things they know.

Evaluated by: qualitative studies (e.g., cognitive walkthroughts). 

Here, we're focusing on one user, and one task.

_Participant_

Users are participants in some environment (other tasks/interfaces/people are also considered here; what's the importance of our task relative to everything else
that's going on with them?). 

Interface must: fit with their context. They must be able to interact with the system in the context that they need to use it in.

Evaluated by: in-situ studies, which allow us to observe people in the context of real-world situations. 

### Views of user - origins

The processor: stems from behaviourism, established by John Watson (little Albert); Pavlov (dogs); and Skinner (operant conditioning, in rats). This only pertains to 
looking at behaviour in HCI — which designs lead to the appropriate behaviours?

The predictor: cognitive psych is the origin. Perception, attention, memory, etc. Began, to some degree, with Descartes and Kant (is knowledge inborn or developed?), but
in psychology, grew in the 1950s. Cognitivist thinkers: Susan Carey, Joh McCarthy, Marvin Minsky, Allen Newell, and Herbert Simon. 

For us in HCI, we care about what the user is thinking; what do they predict is the right action to take? We want to get into the user's head and know how they
predict  the interface will function. 

The participant: similar to the functinalism view of psychology (examining mental behaviours in the context of braoder environments), and system psychology (focuses on 
human behaviour in larger systems). While the processor and predictor views, in HCI, emphasize the individual and the task within a vaccuum, the participant view
acknowledges that the user is embedded within a large system. It sees the user and interface within a larger system. 

Influential thinkers: Edwin Hutchins, Lucy Suchman, Gavriel Salomon, and Bonnie Nardi.

### Designign with the 3 views (CASE STUDY)

Processor model: try multiple interfaces, and check time to completion + accuracy. 
Benefits:
- Broadly, makes various design elements easier to compare, hence: 
- allows for objective comparisons between modes of interaction (voice?) and interfaces 
- we can use previously existing data (there may have been experiments done on similar actions previously)


Cons: 
- Mainly, we don't see the reasons for disparities we observe between interfaces (no causality)
- Thus, we can't differentiate output by expertise (especially for novices)
- As a result, this model is good for optimization, rather than redesign.

Predictor mode:
Benefits:
1. More complete picture of interaction, because we can ask them about thought process and expectation
2. Can target different levels of expertise and examine the differences in their knowledge/understanding/cognition

Cons:
1. Analysis can be v. expensive, because it's not structured data. 
2. Analyses can be subject to biases from interview raters
3. Ignores broader interaction context

Participant:
Benefits:
1. Evaluates interaction in context
2. Capture an authentic represenation of user attention

Cons:
1. Expensive to perform (need to set up real-world situations) and analyze
2. Requires real, functional interfaces rather than prototypes
3. Exposes the experiments to a greater number of variables, and consequently making them more difficult to control.

In sum, novices are better evaluated using the predictor model; objective improvements can be evaluated using the processor model; and pre-production
testing can take place using the participant model.

### Good design vs. bad design

Good design considers not just human capability (i.e., considerations of the processor model), but human cognition/experience/knowledge, and their context.

### User experience

UX plays a role at:

1. An individual level (much of what we've discussed)
2. A group level (e.g., Gen X vs. Millennials)
3. A Societal level (e.g., Arab Spring communications)

Quiz:

Use 3 processor levels AND 3 UX levels to design a UI for Morgan, who likes to listen to non-fiction audiobooks on her way to work; she wants to take notes, 
make bookmarks, etc. How would these designs affect her experience as an individual, someone within her group of friends, and in society if this invention caught on?

## Lecture 5

### Intro

The lecture will cover:

- Feedback loops - the ways that we interact with the world, and get feedback on the results of those interactions.

- The gulf of execution (distance between goals, and the actions required to accomplish those goals)

- Gulf of evaluation (gulf between effects of those actions, and the user's understanding of those results)

- Applications of these in our lives

### Feedback cycles

These are everywhere — from reading, to driving a car, to interacting from others. They're how we learn things. We do something, see the result, and adjust 
what we do the next time to yield a different result. Many people define feedback cycles as the hallmark of intelligent behavior, or at least, have 
behaviors derived from feedback cycles as central to their definition (e.g., Intelligence means getting better over time).

### Gulf of execution

Users put input into the interface, and the system outputs output to the user via the interface. Each of these is a challenge. 

The first is called the gulf of execution: how do I know what I can do? How does a user make their goal a reality? How hard is it to do what the user wants to do?

_Gulf of execution_
Components:
1. Need to be able to identify their goal *in the context of the system* (e.g., play a show on demand on Netflix, or play a pre-recorded show on a VCR?); 
there may be a mismatch between a user's understanding that the system's structure.
The user needs to think of their goal in terms of their current system.

2. Need to identify the actions to take in order to make that goal a reality

3. Execute the action, once it's been decided, via the interface.

_Example:_
Microwaves may require multiple explicit keypresses in order to microwave something for 1 minute. There is a shortcut that is less obvious, however — if you press 1 and 
wait, it will microwave something for 1 minute too. In the first case, there is a greater number of actions. In the second, it's more difficult to *identify* the actions
required. Once you know what you want to do, however, it's very quick to execute. Good interfaces allow for both: there's a hard way that you need to discover, and an easy
way that's already visible.

When it comes to microwaving, however, there's another question: we've specified the goal within the confines of the sysem as "heat the food item for one minute." But
maybe that's not the relevant thing to do! Maybe 1 minute won't be enough, or be too much. Some microwaves, however, detect the temperature of the food — that could 
be the superior approach.

_Tips for bridging gulfs of execution_

1. Make actions discoverable — users should be able to find them clearly labelled without the interface
2. Allow the user to mess around; don't include things that will blow their work up, and give them guardrails
3. Be consistent with other tools (e.g., use a disk icon to save, or other common keyboard shortcuts)
4. Know your user - identifying intentions and actions for novice users is most valuable, which is why much is discoverable through menus. For experts, performing the 
    actions themselves is more important, so much like the microwave, it's advantageous to have quicker, more efficient ways of performing actions.
5. Feedforward - feedback on what the user may want to do (e.g., display of refresh icon when you begin to pull screen down on phone; if you don't pull all the way
down, it won't refresh; it's like a little preview) 

_Gulf of evaluation_
User needs to evaluate the new state of the system in response to their actions. 

1. Output: Physical form of the output in the interface (sound? graphic?)
2. Interpretation: Can the user interpret the meaning of the output?
3. Evaluation: Can the user use the interpretation to evaluate whether their goal was accomplished.

Example: There's a thermostat, and I try to turn the heat on. Is the sound that a thermostate emits when you push the button enough to indicate that 
the temperature will change? Nope — we can interpret that we did something, but can't tell if our goal is being reached. 

_Tips for gulfs of evaluation_
1. Give feedback constantly: that input was received; which input was received; etc.
2. Give feedback immediately (even if it's a loading screen)
3. Match the feedback to the action (significant actions >significant feedback)
4. Vary your feedback - visual feedback isn't the only thing to use, and haptic + auditory should also be employed
5. Leverage direct manipulation - dragging things around, or pulling corners to make somethign larger are intuitive actions

### Norman's feedback cycle stages

Norman refers to bridges of execution and evalution, and describes what the user has to do in order to successfully with the world. Specifically, he asks a number
of questions that the bridge must answer:

How easily can users:

Goal - determine the function of the device?
Plan - tell which actions are possible? - what are the alternatives?
Specify - determine the mapping from intent to movement? - Which plan option should I choose?
Perform - actually perform the physical movement - How do I do it?

Perceive - tell what state the system is in? - What happened?
Interpret - tell if system is in the desired state? - What does it mean?
Compare - compare what they interpreted as happening to what they wanted to happen. - How close is this to the result that I was interested in?

We can break Norman's stages up thus:

- reflective : metacognition
- behavioural : deliberation
- visceral : reaction 

### Quiz

Think of a time where you've come across a wide gulf of execution, and a wide gulf of evaluation.

## Lecture 3.1 

We don't want to build creative interfaces for their own sake — novelty should have a purpose. To figure out that purpose, we need to understand user needs. 

### User-centered design
Considers need of the user throughout the process. Oftentimes, however, this isn't how design is done when designing a tool/project. 

How do we do this? 

1. Needfinding - We observe the user's needs in depth, by observing them, and asking them direct questions.
2. Present alternatives to the user to get feedback
3. Evaluate the quality of the design with real working users. 

Again, we can't design good interfaces by applying good rules of thumb alone — we need to test with users.

### Principles of user-centered design

1. The design is based on explicit understanding of users, tasks, and environments (needfinding on user, how, and where they perform tasks!).
2. Users are involved throughout the design and development (so much so that on occasion they work with the design team)
3. Design is driven and refined by user-centered evaluation
4. Process is iterative (design continues to be improved, even after the design is released)
5. The design addresses the whole user experience
6. The design team includes multidisciplinary skills and perspectives

### Stakeholders
There are multiple stakeholders in the design. The user is the primary stakeholder, and use our system directly. Secondary stakeholders, meahwhile, 
interact with the output of the user's task in some way. Tertiary stakeholders don't interact with the tool/its output, but are impacted by the tool's 
existence. 

Example:
Tool for teachers to send feedback re: kids to parents. 

Primary: Teachers are the main users of the tool, and enter the data into the tool + send it to the parents.
Secondary: Parents interact with the output of this tool (receive messages from teachers)
Tertiary: Children don't interact with the output of the tool at all, but greater teacher-parent communication impacts them. 

When creating this tool, we need to keep *all three* stakeholders in mind (e.g., helicopter parenting of tertiary user, which is bad).

[QUIZ: places where software engineers, data scientists, or non-technical people were put in charge of creating the interface? How did it go?]

### Design life cycle

4-phase design life cycle.

1. Needfinding - gather comprehensive undrstandin fo the task that the user is trying to perform (5 W questions)
2. Design alternatives - Develop multiple design alternatives - design early ideas on the ways to approach the task
3. Prototyping - develop prorotypes from ideas iwth the most potential, to show/give user; refine, and improve.
4. Evaluation - Get feedback on what works/doesn't work. 

After evaluation, we now have additional feedback which we can incorporate into needfinding, which allows us to create additional design alternatives.

### Design life cycles meet feedback cycles

In a feedback cycle, the user does something to accomplish a task, and the judges, via the output, whether we're successful. In the design life cycle, 
our goal is to help users accomplish their tasks; we brainstorm interfaces which help users accomplish goals, and based on the output of our evaluations, 
we judge whether these were accomplish. We then repeat and continue. 

### Qualitative vs. Quantitative

1. Descriptions of what users do when they're accomplishing a task
2. Measures of how long a task takes to complete, or how many people judge it to be difficult3

Later on, we can judge

3. Preferences of interface aspects
4. Performance on certain tasks


Quantitiative data is the easier one to describe: it
- supports descriptive statistics, formal tests, and comparisons
- make objective comparisons
- come to formal conclusions

Still, this only captures a small class of things, while qualitative captures everything else:

- observations
- experiences
- accounts
- open-ended survey responses

Qualitative gives a broader, and more general picture, but it's harder to generate formal conflusions using it; it's more prone to biases.

Sometimes, we can convert qualitative data into quantitative data through numerical summaries. 

Generally, though, we can say that quantitative data provides the *what*, whereas qualitative data provides the *why*. Initially, in needfinding, 
we may want to use qualitative data to examine the breadth and depth of the problem facing the user, but then use quantitative data when evaluating
the prototype we built. This is referred to as a *mixed method* approach.

### Quantitative data

- Nominal/categorical data - number of instances of different categories
    - single nominal - users can select a single choice
    - multi-nominal - users can select multiple categories
    - binary
    - non-binary 

- Ordinal - similar to categorical data, where data points are ordered (we don't know how big the gaps between these categories are)
    - binary - (e.g., pass vs. fail)
    - non-binary (e.g., Likert scale)

- Interval - The intervals between different options as the same. 
    - discrete 
    - continuous

- Ratio - Data that we can conduct ratio comparisons with.
    - continuous 
    - discrete


### Qualitative data (determined by how it's gathered rather than by its output)

- Transcripts

- Field notes

- Artifacts

This is much more expensive to analyze, and prone to biases. To analyze this, we code it (often to nominal data), and then analyze it. 

The benefit of this is a documented methodology for coding the qualitative data to quantitative data. 


### DESIGN CHALLENGE: Improve the MOOC recording process

### How to execute design life cycle in the context of my interests? 

## Lecture 3.2 - Research ethics

### IRB origins

- Milgram's obedience experiments
- Tuskegee syphilis experiment
- Stanford prison experiment

In response, in 1974 the National Research Act was passed, which instituted IRBs to oversee research in universities. The Belmont report, which came
out in 1979, codified the principles that research needed to follow in order to received public funding.

Belmont report:
1. Benefits to society must outweigh the risks to the individual participants
2. Participants must be selected fairly
3. Participants must be presented with the appropriate informed consent knowledge

Consequently, pros outweight cons, and participants' rights are preserved.

### Value of ethics

Not just bureaucracy - it's also about doing GOOD research:

- helps ensure that only certified individuals perform research on humans
- in the process of ensuring that participants do not feel coerced, we improve the quality of our results, because coercion is associated
with tainted responses 
- ensures that the research we perform is not just sound, but also useful.

### IRBWISE

You can access all your experimental protocols in this UI, and their IRB statuses, in this portal.

### Informed consent

Waivers only apply if the participants' data was already collected/anonymized, we may not need consent. Additionally, if their signature 
is the only thing that identifies them when providing responses to a survey, we may not need it, because their continued participation in it 
is an implied form of consent.

### Research ethics and industry

Industry isn't bound by the constraints of IRB approvals (e.g., Facebook's emotional contagion study).

### Evolving IRB 

Two Facebook employees have written a paper suggesting an IRB for industry research, which heavily leans on the expertise of an external reviewer + external IRB. 

## Lecture 3.3

### First stage of the design lifecycle: Needfinding/requirements gathering

It's essential to come in without any preconceived notions when we begin this — you can't go in with the approach you want to take already in mind. 

1. We'll start by defining who the user is, what they're doing, and what they need.
2. We'll go through several methods of generating answers to these questions
3. Figure out how we formalize the data that we gather into a shareable model of the task

### Data inventory
1. Who are the users?
    - demographics
    - experience/expertise
2. Where are the users (environment)?
3. What is the context of the task + competing for the user's attention?
4. What are their goals?
5. What do they need (physical info, collaborators, etc.)
6. What are their tasks?
7. What are their subtasks?

### Problem space

We first want to understand the scope of the space we're looking at. We want to look at the larger problem space, or the problem that we're trying to solve. 
Is the problem the fact that your computer screen's keyboard is too shallow? No! More likely, it's the fact that you're trying to communicate and record information
very quickly, because you're transcribing something. Rather than redesigining the keyboard, perhaps we can give users an interface which includes the transcription, 
and eases their selection of the correct words.

### User 

We want to gather the full range of information about the users. Are they kids or adults? Experts of novices (and in which context)? The Sony Walkman was such as 
success because they not only identified the users, but marketed it to each of those specifically.

### Tips to avoid bias in needfinding

1. Confirmation bias - we see what we want to see. 
    - avoid this by trying to look for signs that we're wrong.
2. Observer bias - we may influence users while we're observing them and getting information from them using our demeanor and language. 
    - avoid this by preventing experimenters with skin in the game re: supporting a theory/UI from running the actual experiment on the day.
    - avoid this by heaviliy scripting interactions with users. 
3. Social desirability bias 
    - we can avoid this by recording objective observations
    - hide what desirable outcome is
    - conduct naturalistic observations
4. Voluntary response bias - people who feel strongly generally answer voluntary users. We don't want to oversample extreme views!
    - avoid this bias by not giving away the contents of the survey before people begin taking it
    - check conclusions with other methods
5. Recall bias
    - study tasks in context
    - conduct interviews during the activity itself

We can also engage in multiple forms of needfinding, which will help reduce all of these biases.

### Naturalistic observation

- Can't interact with those people
- Can't find out what those people are thinking

Still, this is great for a prelim exploration that helps define the problem space.

5 tips:

1. Take notes
2. Start specific (individual actions), and then abstract. 
3. Spread out the observation sessions across times and contexts
4. Find a partner (inter-rater reliability)
5. Look for questions. Naturalistic observations help inform the questions that you'll investigate further.

### Participant observation.

Can participate in the task and see what YOU need. Still, remember that you're not the user — you should merely use your experienc eto see what 
to ask users going forward.

### Hacks and workarounds - beginning to ask why.

How do users break out of an interface to accomplish tasks? If you're looking to make already-existing tasks easier, getting into users' heads and seeing how
they're using traditional interfaces in non-traditional ways is very helpful.

Example: looking at a person's workspace can be employed for this. Someone has post-it notes on a desk that already has multiple monitors set up — why are 
users doing it? What purpose is it accomplishing? How can they possibly need more real estate? Since post-it notes can't be covered up, and are visible even
when the computer is off, using them gets you around the interface's issues.

Remember: you need to ask WHY

### Errors - beginning to ask why.

When making iterative improvements, we often look to errors that users make with the tools that they currently have available. We can fix those errors, but
also, we can use those errors to understand the users' mental model. Mistakes are made because we don't have strong mental models of how something works; if you're
watching someone make errors, you can ask them about it. 

To do so, we may be best off to get people to recruit people to come in and discuss their usual routine. We can also recruit people to allow us to 
document their exercise routine.

### Apprenticeship and ethnography

For particularly complex tasks, we may need to become experts ourselves in order to help redesign the interface. This is about integrating yourself into a specific
knowledge area, and becoming an expert in it.

Example: need to see what video editors do and how their work flows in order to redesign an interface to help them out. 

### Interviews and focus groups

Interviews are useful in fidning out reL what a user is thinking while engaging in tasks; you can also do this with multiple users, in a focus group setting. 

While focus groups may yield more viewpoints, they also risk having a uniformity of ideas as the output, since indidivudals affect each other. 

5 tipes for interviews:

1. 6 Ws when writing your questions: who, what, where, when, why, how (i.e., open-ended, semi-srtuctured.)
    [Are people most interested in why? or how? or when? If people only want to ask yes/no questions, is that our tendency to 
    seek confirmatory evidence at play? Which communities are most open to discussion and understanding rather than forcing their viewpoints on people?]

2. Be aware of bias - make sure you're not predisposing participants to certain views

3. Listen and gather data, rather than have a conversation! Participant should be doing most of the talking.

4. Organize the interview (and get ready to keep it on track): 
    - intro
    - lighter question, to build trust
    - body of the interview
    - summary of the interview

5. Practice the interview!

### Think-aloud protocols (think aloud and post-event)

We're asking users to think out loud while engaging in the task. We can give users voice or video recording devices to document what they're doing in-situ.
Note that occasionally, users may change their behaviours simlpy because they're documenting it. To prevent this, we can use a *post-event protcol* where instead of 
thinking aloud during the task, users repor their reasoning and thought processes immediately after they've completed the task.

### Surveys

Most other methods we've mentioned require a large amount of effort for a relatively small quantity of very deep, unstructured data. A survey, 
is a much lower-effort method of needfinding, as well as one which is quick to disseminate using the internet. 

5 tip

1. Less is more - biggest mistake is asking too much, which affects both the response rate and the reliabilitiy of the data. 
2. Be aware of bias - look at the phrasing of the questions.
3. Tie them to your inventory - make sure your goals have been operationalized as questions.
4. Test it out - test in small groups before sending to the full sample
5. Iterate - survey design is like interface design: test, evaluate, and revise. Give participants a chance to give feedback so that you can improve it in future.

### Writing good survey questions

Surveys shoudl be:

#### Clear
Want to make sure the user understands what we're asking (e.g., if there's a numeric scale, we should label all the numbers with their categories; 
we should also make sure the  categories are equally balanced)

#### Concise
Ask questions using simple language that respondents can understand.

#### Specific
Avoid questions that pertain to v. broad ideas, because we can't be sure which aspect the user is responding to. We should also avoid
questions that may result in internal conflict — e.g., not "how satisfied with your food" but "how satisfied you with your appetizer's temperature"
and "how satisfied were you with your appetizer's smell?"

#### Expressive
This means that we should allow the user to be expressive: questions shoudl emphasize their personal opinions, so that they feel free to be emphatic
or critical. When possible, we should use ranges, instead of yes/no questions. 

e.g., "Do you use socila media?" >> "How many hours, in the past 7 days, have you used a social interaction app (apps include X, Y, Z)"

We should always provide lots of levels when describing frequency or agreement.

When possible, it's also useful to allow users to make multiple selections. For questions that are nominal/categorical, it can be useful to 
give users an option to add other categories for things we didn't anticipate. We don't want to bias them towards our pre-established selections. 

#### Unbiased
- It's good to leave open-ended questions open in the first few iterations of a survey — don't include categories, because you may impact the respondent's answers. Only include catgories if you've ascertained that you only ever really get a limited number of answers.

- Avoid leading questions; avoid loaded (e.g., how much time have you *wasted* on social media vs. how much time *spent* on social media) questions.

#### Usable

- Provide a progress bar and adjust a user's expectation accordingly. Users may quit the survey if they anticipate it'll take ages. 

- Ensure your page lengths are consistent, becaues the user will be discouraged and annoyed. This helps manage their expectations.

- Order your questions logically/into topics. 

- At the end of the survey, alert users to unanswered questions. 

- Preview the survey yourself to make sure it looks the way you wanted.


### Other data gathering methods

#### Existing UI evaluation

If you want to design a new system for ordering takeout, you might evaluate the extant systems to do this.

#### Product reviews

If your'e trying to develop a tool that people are already addressing, you can examine user reviews to see what people like/dislike about 
extant products.

#### Data logs

If you're looking at a task that involves a lot of data-logging, you can get data logs and find trends both within and across users.

[DESIGN A NEEDFINDING APPROACH TO UNDERSTAND THE TASK OF BOOK READING ALONE]

- go to the library and look at people
- challenge here is that there are almost too many users to choose from

### Iterative needfinding

Needfinding can be an iterative exercise, by using multiple needfinding methods to inform the other forms of needfinding that we do. 

### Revising the inventory

Needfinding gathers a vast amount of data about your users. We need to pay special attention to areas where data conflict, and revisit 
the data inventory we wantefd to gather initially:

1. who are users?
2. where are the users?
3. what is the context of the task?
4. what are users' goals?
5. right now, what do users need?
6. what are their tasks?
7. what are their substasks?

### Representing the need

There are a number of ways to formalize the users' needs. We can:

- create step by step task outline of users accomplishing tasks, and break those down to the substasks required for each higher-level task.
- We can further develop that task outline into a hierarchical network. 
- We can then augment this into a diagram of the structural components of the system
and how they interact together.
- we can develop this into a flow chart equipped with decision-making points or points of interruptions.

### Defining requirements

This is the final stage of needfinding is defining hte requiremnts that our final interface must use: they are *specific* and *evaluatable*, 
such as 
- functionality - what the interace can actually do
- usability - how certain user interactions must work 
- learnability - how fast a user can learn to use the interface
- accessibility - who can use the interface
- compatibility - which other technologies the interface must be compatible with 
- compliance - how interface protects user privacy
- cost 
 
 ## Lecture 2.3 - Two applicatinos of good feedback cycles
 
 1. Direct Manipulation

 Direct manipulation - user should feel, as much as possible, that they're directly controlling the object of their task (e.g., dragging corners of image in opposite 
 directions ino rder to enlarge it). 
 
 2. At its best, the interface disappears what we call an invisible interface — and all a user's time is spent on 
 interacting with the task, rather than figuring out interface specifics. 

### Direct Manipulation, the desktop metaphor

We aim to reduce the gulfs of execution and evaluation as much as possible. 

To understand direct manipulation, let's talk about the desktop metaphor. We move items on our desktop; we put them in a folder. 
We do this physically. If computer files/folders are to mimic files/folders in the real world, shouldn't we leverage 
real world expectation?

Ideally, the action of moving them on our computer mimics doing so in real life. Of course, that's how it is now, but previously,
this was all done using the text-based command line (it can be hard to do this!). There's nothing natural about this, and 
it's gibberish for novices!

With the mouse, the action of moving files and folders becomes much closer in nature to direct, physical world manipulation. 

Of course, the gulfs of execution/evaluation are still present using today's desktop setup — the fact that people have to realize
that moving a mouse moves the cursor on the display is testament to this. Using a touch-screen, however, the direct manipulation
is somewhat closer to employing direct manipulation, with users physically dragging icons to their desired locations. 


### "Direct manipulation interfaces" - contingent on immediate feedback that maps directly to the interaction

In 1985, direct manipulation was becoming more and more common as a design strategy, which 
is when this paper came out. Norman and Hutchins identified two key aspects of direct 
manipulation:

1. Distance: distance between the user's goals and the system itself (idea of gulfs of execution/evaluation). They wrote that the feeling of "directness" is inversely proportional to the amount of cognitive effort it takes to manipulate and evaluate the system.

    a. semantic distance: the difference between the user's goals and their expression within the system (i.e., how hard it is to know what to do)

    b. articulatory distance: distance beteween knowing what to do, and doing it (i.e., between
    semantic distance and using the system successfully to accomplish that task)

2. Direct engagement (this is what seofts direct manipulation apart) - The systems that best exemplify direct manipulation all give the feeling that the 
users are directly engaged with the objects that we intent to manipulate. This is what takes a good feedback cycle, and makes it an instance of direct 
manipulation (it's a method for shortening the distance between the gulfs of execution and evaluation). 

NB: VR is allowing us to bring direct manipulation to tasks we've never been able to bring them to before (e.g., architectural design), but doesn't 
have a number of necessary feedback mechanisms that we'd use in the real world, like physical force that we'd use in our daily life.

NB: direct manipulation doesn't aim to match the physical metaphor of, say, a desktop — it aims to match the naturally expected ways of interacting with 
an object. The resulting question, however, is then this: at what point is behaviour, such as pinching to change the size of an object or scrolling to 
zoom, expected? To what degree is direct manipulation, a phenomemon that's not framed as being culturally-dependent, an outgrowth of our cultural 
context? Since there are clear cultural supports required for the idea of direct manipulation to hold, does direct manipulation simply refer to 
"things that we expect to do physically, with our body, do directly manipulate the task object, rather than use an indirect method of doing so, 
like a touch pad?

### Making indirect manipulation direct
On a mac, putting your fingers in the middle of the touch pad and quickly moving them outwards moves all your windows off screen, to reveal your desktop. There's
nothign inherently representatinve of what you would do physically in this scenario if you were handling a touch screen about this gesture; maybe
swiping your whole hand across the touch pad would be more representative, because it's as if you're swiping all of the things off screen, across the 
desktop. What helps make this interaction more direct in the present case is the animation of the windows flying off screen. By using clever animations
as secondary channels of communication, we can help reinforce the idea of direct manipulation in indirect interfaces.

### Invisible interfaces

Whether by using direct manipulation, or the user's time spent learning, the goal is for the user to spend no time thinking about the interface itself. Once
the user spends no time thinking about how to use the interface in order to accomplish the task, it becomes invisible.

### Invisibility by learning

Just because an interface is invisible, doesn't mean it's great. With enough practice, many users become so proficient with a large number of lousy interfaces;
take driving, for example! Driving is important enough to have a high learning curve and to take months to learn, but for our interfaces, we don't have the 
luxury of their buy-in to learn what we create. 

### Invisibility by design

People tend to underestimate the complexity of HCI because when it's done right, people don't realize that the interface is invisible. Numerous principles help
create invisible interfaces:

-leverage prior expectations
-give quick feedback
-have the internal mental model match the system

In fact, if invisibility is key for good HCI design, this course can be titled "Creating invisible interfaces"

### 5 tips for creating invisible interfaces

1. Use affordances - i.e., visual design of interface employs the standard "visual language" of interfaces (e.g., buttons are for pressing), 
which helps makes interfaces much easier to understand (shortens the gulf of execution/semantic AND articulatory distance)

2. Know your user - invisibility means different things to different people/users of various levels of expertise (some users want things to be complicated
but invisible once their level of expertise is sufficiently high)

3. Differentiate your user - Provide multiple ways of accomplishing tasks, for both experts and novices (actions via GUI AND via a shortcut using a keyboard)

4. Let your interface teach - ideally, the interface itself will teach users to use it more efficiently rather than needing them to read manuals (e.g.,
the GUI also displays the keyboard shortcuts that they would use to accomplish that samea action).

5. Talk to your user - the best thing you can do is talk to the user, especially while they're using their interface. Note whether they're talking about
the task or the interface — if the interface, that means it's pretty visible!

[QUIZ] How do we create an invisible interface for a remote control that's easier to use than the standard clunky ones?

## Lecture 2.4: Human abilities

It's important to know what people can do. We'll look at 3 systems:

1. Input:  How stimuli are sensed and perceived inside the mind
2. Processing: how input is stored, and how we reason over the input we receive
3. Output: how the brain controls an individual's actions out in the world

### Information processing
What can people do, physically, cognitively, etc? We'll look at the ways that people make sense of input, and the ways that they act in the world; 
we'll look at them in the same way that we looked at the computer, using the processing model.

### Sensation and perception: visual

The most important details can be seen using the center of the eye, which suggests that the most important details should be placed in the center of the user's
view. Peripheral vision is good for detecting motion, but not color or dtail; we can use the periphery for alerts. As a woman, a user has a 1/200 chance
of being color blind. 1/12 men are color blind. We can use color to emphasize parts of the system, but color shouldn't play a pivotal role in the system.

Sight is directional, so if the user is looking the wrong way/has eyes closed, they'll miss visual feedback. Older users have lower visual acuity, so font
size will be important (and potentially adjustable for multiple types of users)

### Auditory

- We can discern noise based on pitch and loudness. 
- We're good at localizing sound
- We can tell the difference between a nearby quiet sound and a louder, far-off sound, even if their pitches and loudnesses as they reach our ears.
- Hearing isn't directional, so it's harder to filter our auditory information; this may lead to overwhelming the user unless we're thoughtful about our alerts.

### Haptic

- Skin can touch when things are touching it
- It can detect pressure, vibration, and temperature
- It's hard to filter out touch feedack; touch feedback is available only to the person that's being touched. 
- Traditinally, haptic feedback has been natural, but with VR, touch needs to be designed into the system

[QUIZ - design a system to alert a user, in different contexts, without disturbing others, of a received text message]

### Memory - perceptual store

Perceptual store/working memory - lasts less than a second; 

Has 3 parts: 

1. Visuospatial sketchpad, holds visual information for active manipulation
2. Phonological loop - stores sounds and speech you've just heard
3. Episodic buffer -integrates info from the first 2 systems and chronologically orders them

These are controlled by a central executive.

If you have domain expertise, you can interpret the things in the perceptual store. Indeed, experts are much better at remembering real chessboards, but are
just as bad as anyone else when it comes to remembering fake ones. Why? Because they have meaning! Meaning, when it's computed, helps delay the decay of 
the perceptual buffer.

### Memory - short-term memory

It's believed that we can store 4-5 chunks of items in short-term memory at a time. Note that this refers to chunks — items that are grouped.

Implications for HCI:
- we don't want to force users to remember more than 4-5 items at a time
- we can help users remember more things by chunking them
- when possible, we should leverage recognition over recall

### LTM

- seemingly limitless
- difficult to store things there, but generally, need to store things in STM multiple times before it is then committed to LTM.


### Cognition: Learning

When we design interfaces, we're hoping that the user has to learn as little as possible; we can also hope that users learn in the most efficient way. 

There are 2 types of learning:
1. procedural - how to do something, a skill (pasting from the clipboard in MS Word)
2. declarative - knowledge about something, something that you can answer when asked (knowing that Ctrl+V pastes from the clipbaord in MS Word)

Declarative knowledge is how we communicate, but procedural knowledge is the focus of HCI — we want people to be unconsciously competent about what we're doing.
It's difficult to translate subsconsious procedural knowledge into conscious, declarative knowledge. As designers of interfaces, we're experts in our domains, 
so we need to be aware of our bias towards our own procedural knowledge.

### Cognitive load

- The brain has finite resources, so disrtactions and stress can make it difficult to complete tasks. In our case, we need to make sure that the interface
requires few resources, to leave enough for the user to accomplish their task. 

- We also need to think about what else is competing for the user's cognitive resources while they're using our interface, and take the amount of 
available cognitive resources at that moment into account.

[QUIZ - think of an instance that required a high cognitive load, and how could the UI have helped you with this problem]

Programming is one such task, but IDEs can help with this (in-line automated error checking is one such example). It helps to describe this 
as distributing the cog. load more evenly between the different components of the system (user and computer), which we'll get into when we discuss distributed
cognition.

### 5 tips for reducing cognitive load
- use multiple modalities (visual and verbal, for example)
- let modalities complement each other - focus on letting each modality support, illustrate, or explain the other (visual and aural)
- let the user control the pace - built-in timers aren't a great idea, because it dictates the pace on the user. Let the user control the pace!
- Emphasize eessential content and minimize clutter - the principle of discoverability says that we want users to find functions available to them, 
so we should design our interfaces so that they emphasize common actions, but allow access to all available options.
- Offload tasks - look at what the user has to do or remember, and ask if you can offload part of that to the interface; if there's a manual task, can 
it be automated?

### Motor system

In designing interfaces, we're also interested in motor input — what a user can/can't do physically (speed, power, precision, etc.). 

## Lecture 3.4

Once we've done some research on user needs, we can move on to phase 2 of the design life cycle: design alternatives. We'll talk about how we generate ideas for designs in this lesson, and then how to pursue those.

### Second biggest mistake

The biggest mistake designers can make is designing without undesrtanding a task. The second biggest is settling on a single design idea, or a single genre of idea, too early. e.g., if you're trying to improve something, don't necessarily stick to the accepted interface style; voice and gesture controls may be good alternatives that you don't consider. 

It's often thought of a mistake to generate many ideas for interfaces since you don't end up using them, but that's not true. You explore the problem more, you think about user needs more creatively, and eventually, your final interface ends up being a combination of these things. 

### The Design Space

The design space is the area in which we design solutions to the problem we outlined in our problem space. Our goal during the design alternatives phase is to explore the design space, incorporating many types of interfaces and their modalities into the design space.

### Individual brainstorming 1

Brainstorming ususally starts better individually; generate many, thinking about designing with various different interactions; think about the various contexts that people use those devices in; think about the novices/experts, and other demographic factors. What are the "worst" ideas you can think of? The goal is to generate ideas, NOT to come up with the best ones. 

### Individual brainstorming , 5 tips

1. Write down the core problem and keep it visible to stay focused on the end goal
2. Constrain yourself - decide you want at least one ideas in a number of different categories; think of things that are physically impossible
3. Aim for 20 ideas
4. Take a break
5. Divide and conquer - dividing the task into smaller problems and brainstorming solutions to those smaller tasks is a good idea


### Group brainstorming challenges

1. Social loafing - it's easier to slack off when you're with others
2. Conformity - people want to agree, and convergent thinking isn't necessarily helpful
3. Production blocking - some individuals dominate conversations, and their ideas may win out because of how they're communicated, rather than because they're good.
4. Performance matching - people tend to converge in terms of passion and performance, which saps the energy of those who are super enthusiastic at the start, because they match the many people that are less enthusiastic as they go.
5. Power dynamics/biases - no matter how supportive or encouraging a boss may be, there's a hint that their suggestions are best, and they shoudl be adapted.

### Group brainstorming - how do we get good at this?

1. Expressiveness - any idea, no matter how wild, are shared
2. Nonevaluation - don't criticize!
3. Quantity - the more you have, the better!
4. Buildig - build on others' ideas

Some additional rules:

5. Stay focused on your goal
6. No explaining ideas (i.e., don't justify)
7. When you hit a roadblock, revisit the problem
8. Encourage others

These rules are only effective when everyone participates, so encourage that people do. 

### Tips on group brainstorming as a whole

1. Go through everyone's individual ideas after they've already brainstormed them
2. Find the optimal size - shouldn't involve more than 5 people, otherwise social loafing can set in
3. Set clear rules for communication (no production blocking)
4. Set clear expectations - say when a session will wrap (either time-based or based on the number of generated ideas), allowing them to maintain their enthusiasm
5. End with ideas, NOT decisions. Let ideas percolate before you decide what you'll use at a later session. 

### Fleshing out ideas - Personas

We createa characters to represent our users: we siulate her employment, her family, her interests, her expertise level, her background — all the things we need to know how they'll use the product. We should create 3-4 of them, each being different to the others, to represent the range of real people that may use our product. 

### Fleshing out ideas - user profiles 

Personas give a small number of stereotypes that we can use in our mental calculations. It can also, however, be useful to generate a large number of user profiles to generate the full design space. We do this by defining a number of variables about our users, and their values; this defines the world of possible user possibilities. 

e.g.,

- exercise expertise
- reading level
- motivation
- tech literacy
- usage frequency

Note that you shouldn't decide for EVERBYBODY - because that's just too much.

### Fleshing out ideas - Timelines/journeymaps

We're also intereted in the various timeframes in which users interact with our task/UI; is it the initial moment that someone, say, wants to exercise? Is it when they've downloaded the app? Is it when they're about to go to bed and are thinking about planning out their week?

e.g., if our app is frustrating to use when a user is getting ready to exercise, users with low motivation will likely abandon using the app/exercising altogether. 

### Fleshing out ideas - Scenarios/storyboards

It's interesting to examine the specific scenarios that users will face when using our UI. While timelines are pretty general, scenarios are much more specific.

e.g., Morgan is out jogging and a loud fire truck goes back. How does she rewind the 30 seconds that she's missed? 
- pause, rewind using a touch interface
- wait until it's quiet to say "rewind" for a voice interface
- make the appropriate gesture for a gesture interface

This has recently grown into video mockups of these scenarios.

### Fleshing out ideas - User modelling

We can look at how exactly the user achieves each of their goals in each interface. We can also keep in mind precisely what the user is thinking of as well. This is similar to using personas, except these are more objective views of the user experience. 

### Exploring ideas

1. remove UIs that are wholly technologically or financially feasible
2. Use the user personas, timelines, scenarios, user modelling and using those constraints, end up with a small number of feasible designs

## Lecture 2.5

### The sets

Don Norman outlined 6 design principles. Jakob Nielsen has 10 design heuristics in Usability Incpection Methods. Larry Constantine and Lucy Lockwood outline another 6 principles. Finally, Ronald Mace proposed 7 principles of universal design (this last one is concerned with design of UIs that can be used by anyone).

In this lesson, we go through all the 15 principles in the sets above.

### 1: Discoverability

Is it possible to even figure out what actions are possible, and where and how to perform them? i.e., when the user doesn't know what to do, they should be able to know what to do. Relevant funcitons should be made visible so that the user can discover them rather than read about htem in the docs or via some tutorial. Note that we shouldn't get too crzy: we want to walk the line between discoverability and simplicity.

### Quiz: how to make picking up a call, rejecting a call, taking a selfie, and taking a screenshot discoverable as gestures?

### Simplicity

There's often a tension between discoverability and simplicity, since, if everything is discoverable, it's actually harder to find the things you want and figure out what to do. The design should make simple, common tasks easy. If something is simple, people of different intellectual, cultural, and knowledge backgrounds can understand something (e.g., parking signs in NYC)

### Affordances

This is the relationship between the properties of an object and the capabilities of an agent. The object with an affordance tells the user how to use it — the very design of it tells you how you're supposed to use it. The presence of an affordance is a combinaiton of the object and the user's knowledge (e.g., if you've never seen door handles, it won't be an affordance for you, indicating that it's meant to be pushed/pulled).

### Affordance vocabulary

Norman's affordances: inherent properties of a device (e.g., door handle)

Norman's perceived affordance: the perceived affordance of a door handle is "pushability" — the user perceives the way in which the device is supposed to be used.

Norman's signifier: when the perceived affordance matches the actual affordance (links perceived affordance to actual affordance).

### Mapping

System should speak the user's language in ways familiar to the user, rather than system-specific terms (e.g., cut + copy + paste rather than "delete, duplicate, insert"). Mapping makes clear where the effect of using them is clear, rather than just HOW to use them.

Light switches, for example, have signifiers and affordances, but no mapping to what they actually do. 

[QUIZ: How do we redesign light switches to indicate which light they're mapped to?]

### Perceptibility

User's ability to perceive what's happening inside the system; the design should communicate the state of the interface its current state, regardless of their abilities. 

e.g., a fan chain on a ceiling fan gives no indication of the spin settings of the fan.

### Consistency

Using whatever we use in our design consistently, not just within our design but within the ecosystem of design products that people use. Be consistent with what other people have done. It's good to be consistent both within, and across, interfaces. By convention, we create expectations for users (e.g., indicating hyperlinks with blue underlined text). When design is consistent, it becomes invisible. 

### Flexibility

Accelerators, like hotkeys, help speed up hte interaction of the expert user; user customizability is important for experts. We should support different interactinos that people like naturally rather than forcing them into one approach or another.

### Equity

Design is useful and marketable for people with diverse abilities; we should help all users have the same user experience (both experts and novices), extending all benefits, like privacy and security, to all levels of users. 

e.g., password requirements mean novices must pick complex passwords, while experts in security surely already will — this means that both levels of user will have the same security experience (some concerns that this may clash with flexibility).

### Ease + Comfort

Design can be used efficiently and comfortably, and with a minimum of fatigue, regardless of a user's body size, posture, or mobility.

### Structure

The overall architecture of a user interface: the whole design should be organized in ways which help a user's mental model match the contents of the task (e.g., newspaper front page in a paper copy vs. its website). 

### Constraints

Constaining the user to only allow them to perform only the correct actions (e.g, ask a user if they want to close a window if they click the button). We should stop faulty user input before it's even received. Constaints prevent the user from inputting input that wasn't going to work anyway. 

### Four of Norman's constraints

1. Physical constraints, like 3-pronged plugs, can only be inserted one way; this is a physical constraint. 

2. Cultural constraints, like facing forward on escalators or qeueuing. 

3. Semantic constraints are inherent to a situ ation: the purpose of a rearview mirror is to see behind you, so the purpose is to see behind you. 

4. Logical constraint: self-evident based on the situation at hand. For example: imagine building furniture; once you're almost done, you have 1 hole left with 1 screw. That's a logical constraint. 

[QUIZ - can you think of any times when you've encountered interfaces with constraints in them?]

### Dealing with errors: Tolerance 

There are two principles that help us deal with errors if they do occur. Tolerance means that users shouldn't be at risk of choosing too much damage by mistake: supporting undo/redo is key, and the cost of mistakes should generally be minimized. First, we constrain the user and not let them make errors, but if they do, allow easy recovery.

### Dealing with errors: Feedback

Feedback must be given so that users can understand why the error occured, and how it can be avoided in the future. Feedback must be informative and immediate; they should also suggest a solution. Not only should it be possible to recover from an error, but the system should tell you how to do this. 

### Documentation

One goal of usable design is to avoid documentation altogether, but it can be necessary to provide this. Nowadays, docs list the tasks that you can do/may want do to rather than simply listing all the features.


## Lecture 2.6

### Mental Models

A mental model is an undersatnd of the processes, connections, and relationships in real world systems. Using these, we generate expectations/predictions about the world, and compare these to what actually happens when the event takes place. When reality doesn't match our mental model, it makes us curious, and frustrates us. As designers, we want to ensure that we create mental models that conform to people's expectations. How do we do this?

- design systems that act the way users want them to act
- design systems that teach users how to act

### Mental models and education

These aren't only important in HCI, but also, say, in education. That's relevant, because you're teaching your users how to use your UI, but your challenge is teaching users without being physically present to explain things to them. Good representations (how we achieve this) show the user to do this. 

### Mental models in action

e.g., example of Volvo and Nissan, which let users turn both AC and heat on at the same time, and don't sufficiently constrain the actions they can take.


### 5 tips: Mental models for learnable interfaces

1. Predictability - can the user predict what will happen (e.g., greying out a button helps user predict that that button won't work)

2. Synthesizability - users shouldn't merely be able to predict an action's result BEFORE they perform it, they should also be able to see the sequence of actions that led to the current state (hard in a GUI, but a log of actions in an undo menu, for example, allows us to do this).

3. Familiarity - This idea is similar to affordances. The interface should leverage actions with thiwhc a user is alrady familiar inthe interface.

4. Generalizability - knowledge of one user interface should be transferrable to others (e.g., copy/paste transferring from MS Word to Google Docs)

5. Consistency - Similar tasks/operations should behave the same way (ctrl+x should cut selected text whether text is selected or not, and not close the app if it's not selected)

### Representations

Representations help ensure people develop appropriate mental models. Using good representations is essential. IKEA instructions, for example, are unusually helpful representations of how users are supposed to build the actual furniture.

### Improving representations

- write them out
- visualize the problem
- substitute abstract items for things that have mappings to real life people/objects/animals/concepts that have similar relationships to those you're trying to explain ( squares + circles > sheep + wolves in class exampel) 

### Characteristics of a good representation

1. Make relationships explicit
2. Bring objects and relationships together (e.g., wolves and sheep)
3. Excludes extraneous details
4. Expose natural constraints (e.g., sheep and wolves)

[QUIZ: redesign a circuit breaker!]
- instead of writing representation on a paper and mapping breaker>paper, paper > objects, just write the rooms/devices on the breaker switches themselves
- map out the house on the circuit breaker so that rooms/devices are represented according to their location
- place circuit for each room in that room

### Representations in interfaces

These are incredibly important in UIs, e.g., Google calendar — white space is free time, whereas filled in blocks represent blocked chunks of time/appointments.

### Metaphors and analogies

Leveraging an analogy or a metaphor can help your users understand how your interface works very quickly. The WSJ's site, for example — the front page is a great representation of the online site, so we can use it the WSJ's print front page as an analogy. 

There's an issue, however — when you use an analogy, users don't know when an analogy ends, so you may have to be explicit about that or account for it. We need to pay special attention to the misconceptiosn that these analogies/metaphors produce.

### Design principles revisited

Using analogies/metaphors relates to consistency, since we need to be consistent with the analogies we use to explain our UI.

In the same way that we emphasize affordances, we emphasize users being able to understand how to learn what the UI does without interacting with it. 

Representations are important because they map the interface to the task at hand. It lets users predict the mapping between the interface and the actions in the world.

### New functionality, old interfaces

We always want to leverage old interface concepts with new functionality, but there's a challenge — why are we designing technology if we're not providing a wholly new thing for them to do? A thing which has no prior expectations or understanding? We'll have to, at some point, teach users something wholly new.

### Learning curves

Every interface requires users to learn something. We want a learning curve where the expertise of the user quickly reaches the adequate proficiency level without a significant amount of experience (i.e., rapid learning curve). 

How can we help users reach the requisite proficiency level faster? Generally, if we use adquate representations, as well as affordances and elements from known interfaces which users can generalize from, we can essentially raise their expertise level without any investment on their part in gathering more experience. 

Otherwise, we can focus on helping users grow their expertise as quickly as possible.

### User error: slips and mistakes 

- Slips: when the user has the right mental model, but makes a mistake anyway (e.g., when dealing with a "save your work?" prompt, if you violate conventions of order, or the default selection is wrong, a user will make a slip because your interface pushed them in a direction which violated their assumed norms).

TL;DR  - when the user knows the right thing to do but does the wrong thing anyway

- Mistakes: When the user has the wrong mental model and consequently does the wrong thing. 

### Types of slips

1. Action-based slips - where user performs the wrong action, or the wrong action on the right action on the right object, even though they knew of the right action. 

TL;DR - doing the wrong thing

2. Memory lapse - when the user forgets something that they needed to do.

TL;DR - forgetting to do the right thing.

### Types of mistakes

1. Rule-based mistakes: occur when the user correctly assesses the state of the world, but makes the wrong decision based on it (e.g., the user knowing that they want to save their document, but clickign "No" instead of clicking yes in response to a confusing prompt).

TL;DR - right knowledge, wrong rule.

2. Knowledge-based mistakes: when the user incorrectly assesses the state of the world in the first place (e.g., user didn't realize they made any changes, and hence didn't realize that they needed to save; they applied the right rule based on their knowledge, but had incorrect knowledge).


3. Memory lapse mistakes: similar to memory lapse slips, but focuses on failing to fully execute a plan, rather than forgetting to do it altogether (e.g., user shuts down computer and fails to come back to a particular prompt).


In our designs, we want to:

- prevent routine errors by leveraging consistent practices, like arrangement of menu prompts.

- let the interface offload demands on working memory from user to computer to prevent memory lapse errors

- want to leverage good representations to help users develop the right mental models to minimize rule-based and knowledge-based errors.

- we should also leverage the tolerance principle to make sure the errors don't have dramatic consequences.

### Quiz - slips vs. mistakes

[QUIZ - how can Morgan ensure she doesn't make mistakes in texting?]

### Learned helplessness

What if a user acts on an interface/task, and seems to be unable to change the output of the system using their input? That's learned helplessness: when users feel like they can't do anything with a particular interface while they use it. 

### Quiz: expert blind spot

There are parts of a task that you perform so well that you assume they're obvious, or forget to say when teaching them to someone else — that's an expert blind spot. Since you're teaching a user to use the interface that you've designed when creating UIs, you need to remember that they won't know many of the things you take for granted. 

[QUIZ: name an instance of learned helplessnes, and an instance of expert blind spots]

### Types of mistakes

1. We first discussed mental models (what they are, and how users use them to make sense of a system)
2. Then we talked about how good representations help users achieve strong models
3. We then talked about hwo issues with interfaces can lead to user error: slips and mistakes
4. We then talked about learned helplessness, and bad feedback on user errors, as well as expert blind spots
# Week 1

## Lecture 1.1

- The human doesn't interact with a computer — in actuality, the human interacts with a _task_, through a computer. The goal is to let the user interact as much as possible with the task, 
rather than interacting with the interface/software. The interface should vanish, be seamless.

- e.g., seamless: many video games. Non-seamless: TV + cable remote controls.

- HCI is everywhere

- HCI is a subset of human factors engineering (like industrial design and product design). There are also subdisciplines of HCI: UI design, UX design, interaction design.

- HCI vs. human factors: human factors is interested in designing interactions between products, systems, or devices (both computer and non-computer products, like dimensions of furniture or product). 

- Many products are becoming more and more dependent on computers — consequently, HCI is everywhere, is only getitng more important.

- For many years, HCI had to do with UI design — helping people interact with screens. Consequently, UI design focuses on on-screen interaction, while HCI, broadly, 
applies to any interface. 

- HCI vs. UX design: HCI pertains to *understanding* the interactions between users and computers, whereas as UX design pertains to *dictating* interactions between
users and computers. To design well, you need to *understand* the user. HCI and UX are two symbiotic concepts — we use the results of our designs to conduct research. 

- Human factors engineering is the merging of Engineering (software engineering, in HCI's case) and Psychology. We use our understanding of psychology to inform
the way we design interfaces; we then use those results to better understand the results of our psychology. E.g., experiment re: people organizing incoming 
materials into piles, and designing a UI to mimic this; this UI can help us think about psychology in novel ways.

- Much of HCI is about research: methods research the user, understand their needs, and evaluating the prototypes we create. On the other hand, HCI is also about design:
designing interactions. These two ideas are symbiotic: research informs design, and design informs research. These feeback cycles are an essential part of HCI: using research
to inform designs, and using design to inform research.

- What will we cover? 

Fundamental HCI design principles

Performing user research

Relationship between design and research

HCI applications

## Lecture 1.2

###  Learning goals (what to know)

1. Understanding common principles of HCI
2. Design lifecycle, and particularly, the role of iterations
3. Expanse of the HCI field and its applications in the modern world

### Learning outcome (what to know how to do): Design effective interactions between humans and computers

#### Design

- What's design, exactly?

1. Design- known principles applied to a new problem
1. Design- it's an iterative process where you gather info, prototype, test those designs, and re-evaluate.

We'll jump into both of these in units 2 (principles) and 3 (methods)

#### Effectiveness

Whether something is effective or not depends on the goals at hand. 

- Is the goal usability for the user? 
- Is it research, and deriving insights into human psychology? 
- Is it the goal to change the activity altogether?

#### Between humans and computers

We're interested in designing INTERACTIONS AND TASKS, not the interfaces they use to accomplish them. Take a thermostat: our goal *isn't* to design a thermostat, but
the way *people control temperature in their homes*.


### Learning strategies

We'll learn by example and by reflection. 


 ## Lesson 3

 HCI is now everywhere, because computers are ubiquitous. We'll be discussing HCI in the context of 3 things:

 1. Technology - *emerging* technological capabilities that let us create new user interactions
 2. Ideas - ideas re: the ways that people can interact with interfaces and the world around them
 3. Domains - existing areas that can be significantly disrupted by computer interfaces

###  Technology: VR

This is a wholly new classification of interaction and visualization. Interesting applications:

- Using VR to treat phobias, where people can authentically and realistically confront fears

### Technology: AR

This doesn't replace (like VR) but augments and compliments inputs from the real world. Examples include Google Glass, or Google's translation headphones. Think, additionally,
of the possibilities of AR headsets/phones giving students additional details on paintings custom tailored to students' own interests [THIS COULD BE SUCH A COOL THING!!!]

### Technology: Ubiquitous computing

Deeply related to the internet of things, but today, because microprocessors are v. cheap, computers are EVERYWHERE. Modern HCI is incorporated into every smart device. 
This has led to the rise of wearable tech, like fit bits. 

### Technology: Robotics

while much of the focus on robotics today is on the physical robots, but human-robot interaction is only going to grow. How do we pragmatically equip robots
to interact with humans, and give humans feedback re: robot interaction? Can we create robots that teach things to humans? [POST LINKS TO PUDDING PIECES!!!]

### Technology: Mobile

Yes, screens are small; input methods are imprecise; users are often distracted. Thanks to mobile, we can support experiences from navigation to stargazing. 

### Technology: Context-sensitive computing

Context is an essential part of our communication. To build computers, we must consequently incorporate contet into computing. For example, we use 
our phones differently based on whether we're driving in our cars, or relaxing at home. There's TONS of research to be done here!

### Idea: Gesture-based interaction

We communicate with gestures to a tremendous degree, and this has begun to be incorporated into HCI. Fingers have a huge amount of dexterity, and perhaps
one day we'll be able to detect their muscular movements without needing a keyboard, which would potentially allow us to replace smartphones.

### Idea: Pen- and touch-based interfaces

For the longest time, we directly interacted with the things that we built. Then computers came around, and we needed interfaces. We then returned to 
more direct methods, like touch screens, which allowed interacting with computers with fewer levels of interfaces in between the user and the computer/task. We've 
now moved on to tablet-based interacion to include pens.

### Idea: Information visualization

### Idea: CSCW, computer-supported cooperative work

Distributed teams are an example of this. We can think of designing for two variables: time and place (same + different, time + place). While most often 
this assists people located across dif. times + places, it can also assist those of us who want to collaborate more productively at the same location/time.

### Idea: Social computing

This pertains to how computers affect the ways in which we interact and socialize. Examples of this include emojis, online gaming, Wiki, and dating websites. 

### Domain: Special needs

Computing can help compensate for age, disability, or illness. Another example: how do you communicate data to a blind person (Bruce Walker works on this at GA Tech;
reach out to him?)

### Domain: Education

Interestingly, learning tasks require them to be challenging in order to truly teach students, and HCI can help make certain things more challenging in the 
appropriate way.

### Domain: Healthcare

A central healthcare problem pertains to organizing the vast quantities of information that we generate every day. To make it useful, HCI helps doctors to more easily
explore diagnoses; lets patients see their treatments; or exercise monitoring, or my fitness pal. Virtual reality exercise is already on the way. Therapy is another 
context where this can occur!

### Idea: Security

People hate when security measures get in the way of their tasks; if security isn't usable, it's not useful. HCI can help with this. CAPTCHAs, for example, 
used to have people evaluate images, but now computers can simply detect the mouse movements to see if a human or an algorithm is moving it.

### Idea: Video games

Games are one of the most obvious examples of HCI. In gaming, oftentimes issues with the game are actually HCI issues (task too hard, controls bad, etc.)


## Lesson 4 - Principles of HCI

We focus on users and tasks of HCI; we talk about the role of the interface as a mediating factor; we'll talk about UX more generally.

### Interfaces

Users use these to accomplish tasks — in this course, ones that are computerized. We'll be focused on the relationship between the user and task rather than 
the user and interface, because the key is performing the task. e.g., if you focus on designing the interface for a new thermostat, you'll try to 
place buttons in a nice way. If you're trying to focus on the task, however, you can create new, revolutionary designs, like the Nest thermostat, which focuss on 
regulating temperature.

### 5 tips for identifying a user task

1. Watch REAL Users
2. TALK to them! Ask them what they're thinking, goals, and motives!
3. Start small — start by looking at small, individual interactions
4. Abstract up - working from small observations in #3, try to abstract up to an understanding of which tasks the user actually want to complete.
5. YOU are not the user — you're designing for people who don't have your experience.

### Usefulness and usability - our goals for interfaces

Usefulness - whether or not an interface that allows users to achieve some task. 

Usable - how easy something it is to use (we focus on these with interace).

It's through understanding the task of navigation, for example, that we've offloaded much of the cognitive load from the user (looking at map and figuring out 
where we need to turn at each intersection) to the interface (Google maps, which tells us where we want to turn).

### User roles

The user can play three roles in HCI:

1. The Processor
2. The Predictor
3. The Participant

_The Processor_
A sensory processor: take input in, and spit output out. If we're designing an interface with this in mind, we need to keep human limits in mind: what humans
can sense, what they can store in memory, and what they can physically do in the world. In this case, usability means that the interface is *physically* usable.

Interface must: keep to human limits

How to evaluate: using quantitative experiments.

_The predictor_
We can about the experience, thought process, and knowledge. We want them to predict what will happen in the world as a result of some action they'll take. We 
want them to map input to output. This means we need to get inside their head. 

Interfaces must: fit with human knowledge; help user learn what they don't know, and leverage things they know.

Evaluated by: qualitative studies (e.g., cognitive walkthroughts). 

Here, we're focusing on one user, and one task.

_Participant_

Users are participants in some environment (other tasks/interfaces/people are also considered here; what's the importance of our task relative to everything else
that's going on with them?). 

Interface must: fit with their context. They must be able to interact with the system in the context that they need to use it in.

Evaluated by: in-situ studies, which allow us to observe people in the context of real-world situations. 

### Views of user - origins

The processor: stems from behaviourism, established by John Watson (little Albert); Pavlov (dogs); and Skinner (operant conditioning, in rats). This only pertains to 
looking at behaviour in HCI — which designs lead to the appropriate behaviours?

The predictor: cognitive psych is the origin. Perception, attention, memory, etc. Began, to some degree, with Descartes and Kant (is knowledge inborn or developed?), but
in psychology, grew in the 1950s. Cognitivist thinkers: Susan Carey, Joh McCarthy, Marvin Minsky, Allen Newell, and Herbert Simon. 

For us in HCI, we care about what the user is thinking; what do they predict is the right action to take? We want to get into the user's head and know how they
predict  the interface will function. 

The participant: similar to the functinalism view of psychology (examining mental behaviours in the context of braoder environments), and system psychology (focuses on 
human behaviour in larger systems). While the processor and predictor views, in HCI, emphasize the individual and the task within a vaccuum, the participant view
acknowledges that the user is embedded within a large system. It sees the user and interface within a larger system. 

Influential thinkers: Edwin Hutchins, Lucy Suchman, Gavriel Salomon, and Bonnie Nardi.

### Designign with the 3 views (CASE STUDY)

Processor model: try multiple interfaces, and check time to completion + accuracy. 
Benefits:
- Broadly, makes various design elements easier to compare, hence: 
- allows for objective comparisons between modes of interaction (voice?) and interfaces 
- we can use previously existing data (there may have been experiments done on similar actions previously)


Cons: 
- Mainly, we don't see the reasons for disparities we observe between interfaces (no causality)
- Thus, we can't differentiate output by expertise (especially for novices)
- As a result, this model is good for optimization, rather than redesign.

Predictor mode:
Benefits:
1. More complete picture of interaction, because we can ask them about thought process and expectation
2. Can target different levels of expertise and examine the differences in their knowledge/understanding/cognition

Cons:
1. Analysis can be v. expensive, because it's not structured data. 
2. Analyses can be subject to biases from interview raters
3. Ignores broader interaction context

Participant:
Benefits:
1. Evaluates interaction in context
2. Capture an authentic represenation of user attention

Cons:
1. Expensive to perform (need to set up real-world situations) and analyze
2. Requires real, functional interfaces rather than prototypes
3. Exposes the experiments to a greater number of variables, and consequently making them more difficult to control.

In sum, novices are better evaluated using the predictor model; objective improvements can be evaluated using the processor model; and pre-production
testing can take place using the participant model.

### Good design vs. bad design

Good design considers not just human capability (i.e., considerations of the processor model), but human cognition/experience/knowledge, and their context.

### User experience

UX plays a role at:

1. An individual level (much of what we've discussed)
2. A group level (e.g., Gen X vs. Millennials)
3. A Societal level (e.g., Arab Spring communications)

Quiz:

Use 3 processor levels AND 3 UX levels to design a UI for Morgan, who likes to listen to non-fiction audiobooks on her way to work; she wants to take notes, 
make bookmarks, etc. How would these designs affect her experience as an individual, someone within her group of friends, and in society if this invention caught on?

## Lecture 5

### Intro

The lecture will cover:

- Feedback loops - the ways that we interact with the world, and get feedback on the results of those interactions.

- The gulf of execution (distance between goals, and the actions required to accomplish those goals)

- Gulf of evaluation (gulf between effects of those actions, and the user's understanding of those results)

- Applications of these in our lives

### Feedback cycles

These are everywhere — from reading, to driving a car, to interacting from others. They're how we learn things. We do something, see the result, and adjust 
what we do the next time to yield a different result. Many people define feedback cycles as the hallmark of intelligent behavior, or at least, have 
behaviors derived from feedback cycles as central to their definition (e.g., Intelligence means getting better over time).

### Gulf of execution

Users put input into the interface, and the system outputs output to the user via the interface. Each of these is a challenge. 

The first is called the gulf of execution: how do I know what I can do? How does a user make their goal a reality? How hard is it to do what the user wants to do?

_Gulf of execution_
Components:
1. Need to be able to identify their goal *in the context of the system* (e.g., play a show on demand on Netflix, or play a pre-recorded show on a VCR?); 
there may be a mismatch between a user's understanding that the system's structure.
The user needs to think of their goal in terms of their current system.

2. Need to identify the actions to take in order to make that goal a reality

3. Execute the action, once it's been decided, via the interface.

_Example:_
Microwaves may require multiple explicit keypresses in order to microwave something for 1 minute. There is a shortcut that is less obvious, however — if you press 1 and 
wait, it will microwave something for 1 minute too. In the first case, there is a greater number of actions. In the second, it's more difficult to *identify* the actions
required. Once you know what you want to do, however, it's very quick to execute. Good interfaces allow for both: there's a hard way that you need to discover, and an easy
way that's already visible.

When it comes to microwaving, however, there's another question: we've specified the goal within the confines of the sysem as "heat the food item for one minute." But
maybe that's not the relevant thing to do! Maybe 1 minute won't be enough, or be too much. Some microwaves, however, detect the temperature of the food — that could 
be the superior approach.

_Tips for bridging gulfs of execution_

1. Make actions discoverable — users should be able to find them clearly labelled without the interface
2. Allow the user to mess around; don't include things that will blow their work up, and give them guardrails
3. Be consistent with other tools (e.g., use a disk icon to save, or other common keyboard shortcuts)
4. Know your user - identifying intentions and actions for novice users is most valuable, which is why much is discoverable through menus. For experts, performing the 
    actions themselves is more important, so much like the microwave, it's advantageous to have quicker, more efficient ways of performing actions.
5. Feedforward - feedback on what the user may want to do (e.g., display of refresh icon when you begin to pull screen down on phone; if you don't pull all the way
down, it won't refresh; it's like a little preview) 

_Gulf of evaluation_
User needs to evaluate the new state of the system in response to their actions. 

1. Output: Physical form of the output in the interface (sound? graphic?)
2. Interpretation: Can the user interpret the meaning of the output?
3. Evaluation: Can the user use the interpretation to evaluate whether their goal was accomplished.

Example: There's a thermostat, and I try to turn the heat on. Is the sound that a thermostate emits when you push the button enough to indicate that 
the temperature will change? Nope — we can interpret that we did something, but can't tell if our goal is being reached. 

_Tips for gulfs of evaluation_
1. Give feedback constantly: that input was received; which input was received; etc.
2. Give feedback immediately (even if it's a loading screen)
3. Match the feedback to the action (significant actions >significant feedback)
4. Vary your feedback - visual feedback isn't the only thing to use, and haptic + auditory should also be employed
5. Leverage direct manipulation - dragging things around, or pulling corners to make somethign larger are intuitive actions

### Norman's feedback cycle stages

Norman refers to bridges of execution and evalution, and describes what the user has to do in order to successfully with the world. Specifically, he asks a number
of questions that the bridge must answer:

How easily can users:

Goal - determine the function of the device?
Plan - tell which actions are possible? - what are the alternatives?
Specify - determine the mapping from intent to movement? - Which plan option should I choose?
Perform - actually perform the physical movement - How do I do it?

Perceive - tell what state the system is in? - What happened?
Interpret - tell if system is in the desired state? - What does it mean?
Compare - compare what they interpreted as happening to what they wanted to happen. - How close is this to the result that I was interested in?

We can break Norman's stages up thus:

- reflective : metacognition
- behavioural : deliberation
- visceral : reaction 

### Quiz

Think of a time where you've come across a wide gulf of execution, and a wide gulf of evaluation.

## Lecture 3.1 

We don't want to build creative interfaces for their own sake — novelty should have a purpose. To figure out that purpose, we need to understand user needs. 

### User-centered design
Considers need of the user throughout the process. Oftentimes, however, this isn't how design is done when designing a tool/project. 

How do we do this? 

1. Needfinding - We observe the user's needs in depth, by observing them, and asking them direct questions.
2. Present alternatives to the user to get feedback
3. Evaluate the quality of the design with real working users. 

Again, we can't design good interfaces by applying good rules of thumb alone — we need to test with users.

### Principles of user-centered design

1. The design is based on explicit understanding of users, tasks, and environments (needfinding on user, how, and where they perform tasks!).
2. Users are involved throughout the design and development (so much so that on occasion they work with the design team)
3. Design is driven and refined by user-centered evaluation
4. Process is iterative (design continues to be improved, even after the design is released)
5. The design addresses the whole user experience
6. The design team includes multidisciplinary skills and perspectives

### Stakeholders
There are multiple stakeholders in the design. The user is the primary stakeholder, and use our system directly. Secondary stakeholders, meahwhile, 
interact with the output of the user's task in some way. Tertiary stakeholders don't interact with the tool/its output, but are impacted by the tool's 
existence. 

Example:
Tool for teachers to send feedback re: kids to parents. 

Primary: Teachers are the main users of the tool, and enter the data into the tool + send it to the parents.
Secondary: Parents interact with the output of this tool (receive messages from teachers)
Tertiary: Children don't interact with the output of the tool at all, but greater teacher-parent communication impacts them. 

When creating this tool, we need to keep *all three* stakeholders in mind (e.g., helicopter parenting of tertiary user, which is bad).

[QUIZ: places where software engineers, data scientists, or non-technical people were put in charge of creating the interface? How did it go?]

### Design life cycle

4-phase design life cycle.

1. Needfinding - gather comprehensive undrstandin fo the task that the user is trying to perform (5 W questions)
2. Design alternatives - Develop multiple design alternatives - design early ideas on the ways to approach the task
3. Prototyping - develop prorotypes from ideas iwth the most potential, to show/give user; refine, and improve.
4. Evaluation - Get feedback on what works/doesn't work. 

After evaluation, we now have additional feedback which we can incorporate into needfinding, which allows us to create additional design alternatives.

### Design life cycles meet feedback cycles

In a feedback cycle, the user does something to accomplish a task, and the judges, via the output, whether we're successful. In the design life cycle, 
our goal is to help users accomplish their tasks; we brainstorm interfaces which help users accomplish goals, and based on the output of our evaluations, 
we judge whether these were accomplish. We then repeat and continue. 

### Qualitative vs. Quantitative

1. Descriptions of what users do when they're accomplishing a task
2. Measures of how long a task takes to complete, or how many people judge it to be difficult3

Later on, we can judge

3. Preferences of interface aspects
4. Performance on certain tasks


Quantitiative data is the easier one to describe: it
- supports descriptive statistics, formal tests, and comparisons
- make objective comparisons
- come to formal conclusions

Still, this only captures a small class of things, while qualitative captures everything else:

- observations
- experiences
- accounts
- open-ended survey responses

Qualitative gives a broader, and more general picture, but it's harder to generate formal conflusions using it; it's more prone to biases.

Sometimes, we can convert qualitative data into quantitative data through numerical summaries. 

Generally, though, we can say that quantitative data provides the *what*, whereas qualitative data provides the *why*. Initially, in needfinding, 
we may want to use qualitative data to examine the breadth and depth of the problem facing the user, but then use quantitative data when evaluating
the prototype we built. This is referred to as a *mixed method* approach.

### Quantitative data

- Nominal/categorical data - number of instances of different categories
    - single nominal - users can select a single choice
    - multi-nominal - users can select multiple categories
    - binary
    - non-binary 

- Ordinal - similar to categorical data, where data points are ordered (we don't know how big the gaps between these categories are)
    - binary - (e.g., pass vs. fail)
    - non-binary (e.g., Likert scale)

- Interval - The intervals between different options as the same. 
    - discrete 
    - continuous

- Ratio - Data that we can conduct ratio comparisons with.
    - continuous 
    - discrete


### Qualitative data (determined by how it's gathered rather than by its output)

- Transcripts

- Field notes

- Artifacts

This is much more expensive to analyze, and prone to biases. To analyze this, we code it (often to nominal data), and then analyze it. 

The benefit of this is a documented methodology for coding the qualitative data to quantitative data. 


### DESIGN CHALLENGE: Improve the MOOC recording process

### How to execute design life cycle in the context of my interests? 

## Lecture 3.2 - Research ethics

### IRB origins

- Milgram's obedience experiments
- Tuskegee syphilis experiment
- Stanford prison experiment

In response, in 1974 the National Research Act was passed, which instituted IRBs to oversee research in universities. The Belmont report, which came
out in 1979, codified the principles that research needed to follow in order to received public funding.

Belmont report:
1. Benefits to society must outweigh the risks to the individual participants
2. Participants must be selected fairly
3. Participants must be presented with the appropriate informed consent knowledge

Consequently, pros outweight cons, and participants' rights are preserved.

### Value of ethics

Not just bureaucracy - it's also about doing GOOD research:

- helps ensure that only certified individuals perform research on humans
- in the process of ensuring that participants do not feel coerced, we improve the quality of our results, because coercion is associated
with tainted responses 
- ensures that the research we perform is not just sound, but also useful.

### IRBWISE

You can access all your experimental protocols in this UI, and their IRB statuses, in this portal.

### Informed consent

Waivers only apply if the participants' data was already collected/anonymized, we may not need consent. Additionally, if their signature 
is the only thing that identifies them when providing responses to a survey, we may not need it, because their continued participation in it 
is an implied form of consent.

### Research ethics and industry

Industry isn't bound by the constraints of IRB approvals (e.g., Facebook's emotional contagion study).

### Evolving IRB 

Two Facebook employees have written a paper suggesting an IRB for industry research, which heavily leans on the expertise of an external reviewer + external IRB. 

## Lecture 3.3

### First stage of the design lifecycle: Needfinding/requirements gathering

It's essential to come in without any preconceived notions when we begin this — you can't go in with the approach you want to take already in mind. 

1. We'll start by defining who the user is, what they're doing, and what they need.
2. We'll go through several methods of generating answers to these questions
3. Figure out how we formalize the data that we gather into a shareable model of the task

### Data inventory
1. Who are the users?
    - demographics
    - experience/expertise
2. Where are the users (environment)?
3. What is the context of the task + competing for the user's attention?
4. What are their goals?
5. What do they need (physical info, collaborators, etc.)
6. What are their tasks?
7. What are their subtasks?

### Problem space

We first want to understand the scope of the space we're looking at. We want to look at the larger problem space, or the problem that we're trying to solve. 
Is the problem the fact that your computer screen's keyboard is too shallow? No! More likely, it's the fact that you're trying to communicate and record information
very quickly, because you're transcribing something. Rather than redesigining the keyboard, perhaps we can give users an interface which includes the transcription, 
and eases their selection of the correct words.

### User 

We want to gather the full range of information about the users. Are they kids or adults? Experts of novices (and in which context)? The Sony Walkman was such as 
success because they not only identified the users, but marketed it to each of those specifically.

### Tips to avoid bias in needfinding

1. Confirmation bias - we see what we want to see. 
    - avoid this by trying to look for signs that we're wrong.
2. Observer bias - we may influence users while we're observing them and getting information from them using our demeanor and language. 
    - avoid this by preventing experimenters with skin in the game re: supporting a theory/UI from running the actual experiment on the day.
    - avoid this by heaviliy scripting interactions with users. 
3. Social desirability bias 
    - we can avoid this by recording objective observations
    - hide what desirable outcome is
    - conduct naturalistic observations
4. Voluntary response bias - people who feel strongly generally answer voluntary users. We don't want to oversample extreme views!
    - avoid this bias by not giving away the contents of the survey before people begin taking it
    - check conclusions with other methods
5. Recall bias
    - study tasks in context
    - conduct interviews during the activity itself

We can also engage in multiple forms of needfinding, which will help reduce all of these biases.

### Naturalistic observation

- Can't interact with those people
- Can't find out what those people are thinking

Still, this is great for a prelim exploration that helps define the problem space.

5 tips:

1. Take notes
2. Start specific (individual actions), and then abstract. 
3. Spread out the observation sessions across times and contexts
4. Find a partner (inter-rater reliability)
5. Look for questions. Naturalistic observations help inform the questions that you'll investigate further.

### Participant observation.

Can participate in the task and see what YOU need. Still, remember that you're not the user — you should merely use your experienc eto see what 
to ask users going forward.

### Hacks and workarounds - beginning to ask why.

How do users break out of an interface to accomplish tasks? If you're looking to make already-existing tasks easier, getting into users' heads and seeing how
they're using traditional interfaces in non-traditional ways is very helpful.

Example: looking at a person's workspace can be employed for this. Someone has post-it notes on a desk that already has multiple monitors set up — why are 
users doing it? What purpose is it accomplishing? How can they possibly need more real estate? Since post-it notes can't be covered up, and are visible even
when the computer is off, using them gets you around the interface's issues.

Remember: you need to ask WHY

### Errors - beginning to ask why.

When making iterative improvements, we often look to errors that users make with the tools that they currently have available. We can fix those errors, but
also, we can use those errors to understand the users' mental model. Mistakes are made because we don't have strong mental models of how something works; if you're
watching someone make errors, you can ask them about it. 

To do so, we may be best off to get people to recruit people to come in and discuss their usual routine. We can also recruit people to allow us to 
document their exercise routine.

### Apprenticeship and ethnography

For particularly complex tasks, we may need to become experts ourselves in order to help redesign the interface. This is about integrating yourself into a specific
knowledge area, and becoming an expert in it.

Example: need to see what video editors do and how their work flows in order to redesign an interface to help them out. 

### Interviews and focus groups

Interviews are useful in fidning out reL what a user is thinking while engaging in tasks; you can also do this with multiple users, in a focus group setting. 

While focus groups may yield more viewpoints, they also risk having a uniformity of ideas as the output, since indidivudals affect each other. 

5 tipes for interviews:

1. 6 Ws when writing your questions: who, what, where, when, why, how (i.e., open-ended, semi-srtuctured.)
    [Are people most interested in why? or how? or when? If people only want to ask yes/no questions, is that our tendency to 
    seek confirmatory evidence at play? Which communities are most open to discussion and understanding rather than forcing their viewpoints on people?]

2. Be aware of bias - make sure you're not predisposing participants to certain views

3. Listen and gather data, rather than have a conversation! Participant should be doing most of the talking.

4. Organize the interview (and get ready to keep it on track): 
    - intro
    - lighter question, to build trust
    - body of the interview
    - summary of the interview

5. Practice the interview!

### Think-aloud protocols (think aloud and post-event)

We're asking users to think out loud while engaging in the task. We can give users voice or video recording devices to document what they're doing in-situ.
Note that occasionally, users may change their behaviours simlpy because they're documenting it. To prevent this, we can use a *post-event protcol* where instead of 
thinking aloud during the task, users repor their reasoning and thought processes immediately after they've completed the task.

### Surveys

Most other methods we've mentioned require a large amount of effort for a relatively small quantity of very deep, unstructured data. A survey, 
is a much lower-effort method of needfinding, as well as one which is quick to disseminate using the internet. 

5 tip

1. Less is more - biggest mistake is asking too much, which affects both the response rate and the reliabilitiy of the data. 
2. Be aware of bias - look at the phrasing of the questions.
3. Tie them to your inventory - make sure your goals have been operationalized as questions.
4. Test it out - test in small groups before sending to the full sample
5. Iterate - survey design is like interface design: test, evaluate, and revise. Give participants a chance to give feedback so that you can improve it in future.

### Writing good survey questions

Surveys shoudl be:

#### Clear
Want to make sure the user understands what we're asking (e.g., if there's a numeric scale, we should label all the numbers with their categories; 
we should also make sure the  categories are equally balanced)

#### Concise
Ask questions using simple language that respondents can understand.

#### Specific
Avoid questions that pertain to v. broad ideas, because we can't be sure which aspect the user is responding to. We should also avoid
questions that may result in internal conflict — e.g., not "how satisfied with your food" but "how satisfied you with your appetizer's temperature"
and "how satisfied were you with your appetizer's smell?"

#### Expressive
This means that we should allow the user to be expressive: questions shoudl emphasize their personal opinions, so that they feel free to be emphatic
or critical. When possible, we should use ranges, instead of yes/no questions. 

e.g., "Do you use socila media?" >> "How many hours, in the past 7 days, have you used a social interaction app (apps include X, Y, Z)"

We should always provide lots of levels when describing frequency or agreement.

When possible, it's also useful to allow users to make multiple selections. For questions that are nominal/categorical, it can be useful to 
give users an option to add other categories for things we didn't anticipate. We don't want to bias them towards our pre-established selections. 

#### Unbiased
- It's good to leave open-ended questions open in the first few iterations of a survey — don't include categories, because you may impact the respondent's answers. Only include catgories if you've ascertained that you only ever really get a limited number of answers.

- Avoid leading questions; avoid loaded (e.g., how much time have you *wasted* on social media vs. how much time *spent* on social media) questions.

#### Usable

- Provide a progress bar and adjust a user's expectation accordingly. Users may quit the survey if they anticipate it'll take ages. 

- Ensure your page lengths are consistent, becaues the user will be discouraged and annoyed. This helps manage their expectations.

- Order your questions logically/into topics. 

- At the end of the survey, alert users to unanswered questions. 

- Preview the survey yourself to make sure it looks the way you wanted.


### Other data gathering methods

#### Existing UI evaluation

If you want to design a new system for ordering takeout, you might evaluate the extant systems to do this.

#### Product reviews

If your'e trying to develop a tool that people are already addressing, you can examine user reviews to see what people like/dislike about 
extant products.

#### Data logs

If you're looking at a task that involves a lot of data-logging, you can get data logs and find trends both within and across users.

[DESIGN A NEEDFINDING APPROACH TO UNDERSTAND THE TASK OF BOOK READING ALONE]

- go to the library and look at people
- challenge here is that there are almost too many users to choose from

### Iterative needfinding

Needfinding can be an iterative exercise, by using multiple needfinding methods to inform the other forms of needfinding that we do. 

### Revising the inventory

Needfinding gathers a vast amount of data about your users. We need to pay special attention to areas where data conflict, and revisit 
the data inventory we wantefd to gather initially:

1. who are users?
2. where are the users?
3. what is the context of the task?
4. what are users' goals?
5. right now, what do users need?
6. what are their tasks?
7. what are their substasks?

### Representing the need

There are a number of ways to formalize the users' needs. We can:

- create step by step task outline of users accomplishing tasks, and break those down to the substasks required for each higher-level task.
- We can further develop that task outline into a hierarchical network. 
- We can then augment this into a diagram of the structural components of the system
and how they interact together.
- we can develop this into a flow chart equipped with decision-making points or points of interruptions.

### Defining requirements

This is the final stage of needfinding is defining hte requiremnts that our final interface must use: they are *specific* and *evaluatable*, 
such as 
- functionality - what the interace can actually do
- usability - how certain user interactions must work 
- learnability - how fast a user can learn to use the interface
- accessibility - who can use the interface
- compatibility - which other technologies the interface must be compatible with 
- compliance - how interface protects user privacy
- cost 
 
 ## Lecture 2.3 - Two applicatinos of good feedback cycles
 
 1. Direct Manipulation

 Direct manipulation - user should feel, as much as possible, that they're directly controlling the object of their task (e.g., dragging corners of image in opposite 
 directions ino rder to enlarge it). 
 
 2. At its best, the interface disappears what we call an invisible interface — and all a user's time is spent on 
 interacting with the task, rather than figuring out interface specifics. 

### Direct Manipulation, the desktop metaphor

We aim to reduce the gulfs of execution and evaluation as much as possible. 

To understand direct manipulation, let's talk about the desktop metaphor. We move items on our desktop; we put them in a folder. 
We do this physically. If computer files/folders are to mimic files/folders in the real world, shouldn't we leverage 
real world expectation?

Ideally, the action of moving them on our computer mimics doing so in real life. Of course, that's how it is now, but previously,
this was all done using the text-based command line (it can be hard to do this!). There's nothing natural about this, and 
it's gibberish for novices!

With the mouse, the action of moving files and folders becomes much closer in nature to direct, physical world manipulation. 

Of course, the gulfs of execution/evaluation are still present using today's desktop setup — the fact that people have to realize
that moving a mouse moves the cursor on the display is testament to this. Using a touch-screen, however, the direct manipulation
is somewhat closer to employing direct manipulation, with users physically dragging icons to their desired locations. 


### "Direct manipulation interfaces" - contingent on immediate feedback that maps directly to the interaction

In 1985, direct manipulation was becoming more and more common as a design strategy, which 
is when this paper came out. Norman and Hutchins identified two key aspects of direct 
manipulation:

1. Distance: distance between the user's goals and the system itself (idea of gulfs of execution/evaluation). They wrote that the feeling of "directness" is inversely proportional to the amount of cognitive effort it takes to manipulate and evaluate the system.

    a. semantic distance: the difference between the user's goals and their expression within the system (i.e., how hard it is to know what to do)

    b. articulatory distance: distance beteween knowing what to do, and doing it (i.e., between
    semantic distance and using the system successfully to accomplish that task)

2. Direct engagement (this is what seofts direct manipulation apart) - The systems that best exemplify direct manipulation all give the feeling that the 
users are directly engaged with the objects that we intent to manipulate. This is what takes a good feedback cycle, and makes it an instance of direct 
manipulation (it's a method for shortening the distance between the gulfs of execution and evaluation). 

NB: VR is allowing us to bring direct manipulation to tasks we've never been able to bring them to before (e.g., architectural design), but doesn't 
have a number of necessary feedback mechanisms that we'd use in the real world, like physical force that we'd use in our daily life.

NB: direct manipulation doesn't aim to match the physical metaphor of, say, a desktop — it aims to match the naturally expected ways of interacting with 
an object. The resulting question, however, is then this: at what point is behaviour, such as pinching to change the size of an object or scrolling to 
zoom, expected? To what degree is direct manipulation, a phenomemon that's not framed as being culturally-dependent, an outgrowth of our cultural 
context? Since there are clear cultural supports required for the idea of direct manipulation to hold, does direct manipulation simply refer to 
"things that we expect to do physically, with our body, do directly manipulate the task object, rather than use an indirect method of doing so, 
like a touch pad?

### Making indirect manipulation direct
On a mac, putting your fingers in the middle of the touch pad and quickly moving them outwards moves all your windows off screen, to reveal your desktop. There's
nothign inherently representatinve of what you would do physically in this scenario if you were handling a touch screen about this gesture; maybe
swiping your whole hand across the touch pad would be more representative, because it's as if you're swiping all of the things off screen, across the 
desktop. What helps make this interaction more direct in the present case is the animation of the windows flying off screen. By using clever animations
as secondary channels of communication, we can help reinforce the idea of direct manipulation in indirect interfaces.

### Invisible interfaces

Whether by using direct manipulation, or the user's time spent learning, the goal is for the user to spend no time thinking about the interface itself. Once
the user spends no time thinking about how to use the interface in order to accomplish the task, it becomes invisible.

### Invisibility by learning

Just because an interface is invisible, doesn't mean it's great. With enough practice, many users become so proficient with a large number of lousy interfaces;
take driving, for example! Driving is important enough to have a high learning curve and to take months to learn, but for our interfaces, we don't have the 
luxury of their buy-in to learn what we create. 

### Invisibility by design

People tend to underestimate the complexity of HCI because when it's done right, people don't realize that the interface is invisible. Numerous principles help
create invisible interfaces:

-leverage prior expectations
-give quick feedback
-have the internal mental model match the system

In fact, if invisibility is key for good HCI design, this course can be titled "Creating invisible interfaces"

### 5 tips for creating invisible interfaces

1. Use affordances - i.e., visual design of interface employs the standard "visual language" of interfaces (e.g., buttons are for pressing), 
which helps makes interfaces much easier to understand (shortens the gulf of execution/semantic AND articulatory distance)

2. Know your user - invisibility means different things to different people/users of various levels of expertise (some users want things to be complicated
but invisible once their level of expertise is sufficiently high)

3. Differentiate your user - Provide multiple ways of accomplishing tasks, for both experts and novices (actions via GUI AND via a shortcut using a keyboard)

4. Let your interface teach - ideally, the interface itself will teach users to use it more efficiently rather than needing them to read manuals (e.g.,
the GUI also displays the keyboard shortcuts that they would use to accomplish that samea action).

5. Talk to your user - the best thing you can do is talk to the user, especially while they're using their interface. Note whether they're talking about
the task or the interface — if the interface, that means it's pretty visible!

[QUIZ] How do we create an invisible interface for a remote control that's easier to use than the standard clunky ones?

## Lecture 2.4: Human abilities

It's important to know what people can do. We'll look at 3 systems:

1. Input:  How stimuli are sensed and perceived inside the mind
2. Processing: how input is stored, and how we reason over the input we receive
3. Output: how the brain controls an individual's actions out in the world

### Information processing
What can people do, physically, cognitively, etc? We'll look at the ways that people make sense of input, and the ways that they act in the world; 
we'll look at them in the same way that we looked at the computer, using the processing model.

### Sensation and perception: visual

The most important details can be seen using the center of the eye, which suggests that the most important details should be placed in the center of the user's
view. Peripheral vision is good for detecting motion, but not color or dtail; we can use the periphery for alerts. As a woman, a user has a 1/200 chance
of being color blind. 1/12 men are color blind. We can use color to emphasize parts of the system, but color shouldn't play a pivotal role in the system.

Sight is directional, so if the user is looking the wrong way/has eyes closed, they'll miss visual feedback. Older users have lower visual acuity, so font
size will be important (and potentially adjustable for multiple types of users)

### Auditory

- We can discern noise based on pitch and loudness. 
- We're good at localizing sound
- We can tell the difference between a nearby quiet sound and a louder, far-off sound, even if their pitches and loudnesses as they reach our ears.
- Hearing isn't directional, so it's harder to filter our auditory information; this may lead to overwhelming the user unless we're thoughtful about our alerts.

### Haptic

- Skin can touch when things are touching it
- It can detect pressure, vibration, and temperature
- It's hard to filter out touch feedack; touch feedback is available only to the person that's being touched. 
- Traditinally, haptic feedback has been natural, but with VR, touch needs to be designed into the system

[QUIZ - design a system to alert a user, in different contexts, without disturbing others, of a received text message]

### Memory - perceptual store

Perceptual store/working memory - lasts less than a second; 

Has 3 parts: 

1. Visuospatial sketchpad, holds visual information for active manipulation
2. Phonological loop - stores sounds and speech you've just heard
3. Episodic buffer -integrates info from the first 2 systems and chronologically orders them

These are controlled by a central executive.

If you have domain expertise, you can interpret the things in the perceptual store. Indeed, experts are much better at remembering real chessboards, but are
just as bad as anyone else when it comes to remembering fake ones. Why? Because they have meaning! Meaning, when it's computed, helps delay the decay of 
the perceptual buffer.

### Memory - short-term memory

It's believed that we can store 4-5 chunks of items in short-term memory at a time. Note that this refers to chunks — items that are grouped.

Implications for HCI:
- we don't want to force users to remember more than 4-5 items at a time
- we can help users remember more things by chunking them
- when possible, we should leverage recognition over recall

### LTM

- seemingly limitless
- difficult to store things there, but generally, need to store things in STM multiple times before it is then committed to LTM.


### Cognition: Learning

When we design interfaces, we're hoping that the user has to learn as little as possible; we can also hope that users learn in the most efficient way. 

There are 2 types of learning:
1. procedural - how to do something, a skill (pasting from the clipboard in MS Word)
2. declarative - knowledge about something, something that you can answer when asked (knowing that Ctrl+V pastes from the clipbaord in MS Word)

Declarative knowledge is how we communicate, but procedural knowledge is the focus of HCI — we want people to be unconsciously competent about what we're doing.
It's difficult to translate subsconsious procedural knowledge into conscious, declarative knowledge. As designers of interfaces, we're experts in our domains, 
so we need to be aware of our bias towards our own procedural knowledge.

### Cognitive load

- The brain has finite resources, so disrtactions and stress can make it difficult to complete tasks. In our case, we need to make sure that the interface
requires few resources, to leave enough for the user to accomplish their task. 

- We also need to think about what else is competing for the user's cognitive resources while they're using our interface, and take the amount of 
available cognitive resources at that moment into account.

[QUIZ - think of an instance that required a high cognitive load, and how could the UI have helped you with this problem]

Programming is one such task, but IDEs can help with this (in-line automated error checking is one such example). It helps to describe this 
as distributing the cog. load more evenly between the different components of the system (user and computer), which we'll get into when we discuss distributed
cognition.

### 5 tips for reducing cognitive load
- use multiple modalities (visual and verbal, for example)
- let modalities complement each other - focus on letting each modality support, illustrate, or explain the other (visual and aural)
- let the user control the pace - built-in timers aren't a great idea, because it dictates the pace on the user. Let the user control the pace!
- Emphasize eessential content and minimize clutter - the principle of discoverability says that we want users to find functions available to them, 
so we should design our interfaces so that they emphasize common actions, but allow access to all available options.
- Offload tasks - look at what the user has to do or remember, and ask if you can offload part of that to the interface; if there's a manual task, can 
it be automated?

### Motor system

In designing interfaces, we're also interested in motor input — what a user can/can't do physically (speed, power, precision, etc.). 

## Lecture 3.4

Once we've done some research on user needs, we can move on to phase 2 of the design life cycle: design alternatives. We'll talk about how we generate ideas for designs in this lesson, and then how to pursue those.

### Second biggest mistake

The biggest mistake designers can make is designing without undesrtanding a task. The second biggest is settling on a single design idea, or a single genre of idea, too early. e.g., if you're trying to improve something, don't necessarily stick to the accepted interface style; voice and gesture controls may be good alternatives that you don't consider. 

It's often thought of a mistake to generate many ideas for interfaces since you don't end up using them, but that's not true. You explore the problem more, you think about user needs more creatively, and eventually, your final interface ends up being a combination of these things. 

### The Design Space

The design space is the area in which we design solutions to the problem we outlined in our problem space. Our goal during the design alternatives phase is to explore the design space, incorporating many types of interfaces and their modalities into the design space.

### Individual brainstorming 1

Brainstorming ususally starts better individually; generate many, thinking about designing with various different interactions; think about the various contexts that people use those devices in; think about the novices/experts, and other demographic factors. What are the "worst" ideas you can think of? The goal is to generate ideas, NOT to come up with the best ones. 

### Individual brainstorming , 5 tips

1. Write down the core problem and keep it visible to stay focused on the end goal
2. Constrain yourself - decide you want at least one ideas in a number of different categories; think of things that are physically impossible
3. Aim for 20 ideas
4. Take a break
5. Divide and conquer - dividing the task into smaller problems and brainstorming solutions to those smaller tasks is a good idea


### Group brainstorming challenges

1. Social loafing - it's easier to slack off when you're with others
2. Conformity - people want to agree, and convergent thinking isn't necessarily helpful
3. Production blocking - some individuals dominate conversations, and their ideas may win out because of how they're communicated, rather than because they're good.
4. Performance matching - people tend to converge in terms of passion and performance, which saps the energy of those who are super enthusiastic at the start, because they match the many people that are less enthusiastic as they go.
5. Power dynamics/biases - no matter how supportive or encouraging a boss may be, there's a hint that their suggestions are best, and they shoudl be adapted.

### Group brainstorming - how do we get good at this?

1. Expressiveness - any idea, no matter how wild, are shared
2. Nonevaluation - don't criticize!
3. Quantity - the more you have, the better!
4. Buildig - build on others' ideas

Some additional rules:

5. Stay focused on your goal
6. No explaining ideas (i.e., don't justify)
7. When you hit a roadblock, revisit the problem
8. Encourage others

These rules are only effective when everyone participates, so encourage that people do. 

### Tips on group brainstorming as a whole

1. Go through everyone's individual ideas after they've already brainstormed them
2. Find the optimal size - shouldn't involve more than 5 people, otherwise social loafing can set in
3. Set clear rules for communication (no production blocking)
4. Set clear expectations - say when a session will wrap (either time-based or based on the number of generated ideas), allowing them to maintain their enthusiasm
5. End with ideas, NOT decisions. Let ideas percolate before you decide what you'll use at a later session. 

### Fleshing out ideas - Personas

We createa characters to represent our users: we siulate her employment, her family, her interests, her expertise level, her background — all the things we need to know how they'll use the product. We should create 3-4 of them, each being different to the others, to represent the range of real people that may use our product. 

### Fleshing out ideas - user profiles 

Personas give a small number of stereotypes that we can use in our mental calculations. It can also, however, be useful to generate a large number of user profiles to generate the full design space. We do this by defining a number of variables about our users, and their values; this defines the world of possible user possibilities. 

e.g.,

- exercise expertise
- reading level
- motivation
- tech literacy
- usage frequency

Note that you shouldn't decide for EVERBYBODY - because that's just too much.

### Fleshing out ideas - Timelines/journeymaps

We're also intereted in the various timeframes in which users interact with our task/UI; is it the initial moment that someone, say, wants to exercise? Is it when they've downloaded the app? Is it when they're about to go to bed and are thinking about planning out their week?

e.g., if our app is frustrating to use when a user is getting ready to exercise, users with low motivation will likely abandon using the app/exercising altogether. 

### Fleshing out ideas - Scenarios/storyboards

It's interesting to examine the specific scenarios that users will face when using our UI. While timelines are pretty general, scenarios are much more specific.

e.g., Morgan is out jogging and a loud fire truck goes back. How does she rewind the 30 seconds that she's missed? 
- pause, rewind using a touch interface
- wait until it's quiet to say "rewind" for a voice interface
- make the appropriate gesture for a gesture interface

This has recently grown into video mockups of these scenarios.

### Fleshing out ideas - User modelling

We can look at how exactly the user achieves each of their goals in each interface. We can also keep in mind precisely what the user is thinking of as well. This is similar to using personas, except these are more objective views of the user experience. 

### Exploring ideas

1. remove UIs that are wholly technologically or financially feasible
2. Use the user personas, timelines, scenarios, user modelling and using those constraints, end up with a small number of feasible designs

## Lecture 2.5

### The sets

Don Norman outlined 6 design principles. Jakob Nielsen has 10 design heuristics in Usability Incpection Methods. Larry Constantine and Lucy Lockwood outline another 6 principles. Finally, Ronald Mace proposed 7 principles of universal design (this last one is concerned with design of UIs that can be used by anyone).

In this lesson, we go through all the 15 principles in the sets above.

### 1: Discoverability

Is it possible to even figure out what actions are possible, and where and how to perform them? i.e., when the user doesn't know what to do, they should be able to know what to do. Relevant funcitons should be made visible so that the user can discover them rather than read about htem in the docs or via some tutorial. Note that we shouldn't get too crzy: we want to walk the line between discoverability and simplicity.

### Quiz: how to make picking up a call, rejecting a call, taking a selfie, and taking a screenshot discoverable as gestures?

### 2: Simplicity

There's often a tension between discoverability and simplicity, since, if everything is discoverable, it's actually harder to find the things you want and figure out what to do. The design should make simple, common tasks easy. If something is simple, people of different intellectual, cultural, and knowledge backgrounds can understand something (e.g., parking signs in NYC)

### 3: Affordances

This is the relationship between the properties of an object and the capabilities of an agent. The object with an affordance tells the user how to use it — the very design of it tells you how you're supposed to use it. The presence of an affordance is a combinaiton of the object and the user's knowledge (e.g., if you've never seen door handles, it won't be an affordance for you, indicating that it's meant to be pushed/pulled).

### Affordance vocabulary

Norman's affordances: inherent properties of a device (e.g., door handle)

Norman's perceived affordance: the perceived affordance of a door handle is "pushability" — the user perceives the way in which the device is supposed to be used.

Norman's signifier: when the perceived affordance matches the actual affordance (links perceived affordance to actual affordance).

### 4: Mapping

System should speak the user's language in ways familiar to the user, rather than system-specific terms (e.g., cut + copy + paste rather than "delete, duplicate, insert"). Mapping makes clear where the effect of using them is clear, rather than just HOW to use them.

Light switches, for example, have signifiers and affordances, but no mapping to what they actually do. 

[QUIZ: How do we redesign light switches to indicate which light they're mapped to?]

### 5: Perceptibility

User's ability to perceive what's happening inside the system; the design should communicate the state of the interface its current state, regardless of their abilities. 

e.g., a fan chain on a ceiling fan gives no indication of the spin settings of the fan.

### 6: Consistency

Using whatever we use in our design consistently, not just within our design but within the ecosystem of design products that people use. Be consistent with what other people have done. It's good to be consistent both within, and across, interfaces. By convention, we create expectations for users (e.g., indicating hyperlinks with blue underlined text). When design is consistent, it becomes invisible. 

### 7: Flexibility

Accelerators, like hotkeys, help speed up hte interaction of the expert user; user customizability is important for experts. We should support different interactinos that people like naturally rather than forcing them into one approach or another.

### 8: Equity

Design is useful and marketable for people with diverse abilities; we should help all users have the same user experience (both experts and novices), extending all benefits, like privacy and security, to all levels of users. 

e.g., password requirements mean novices must pick complex passwords, while experts in security surely already will — this means that both levels of user will have the same security experience (some concerns that this may clash with flexibility).

### 9, 10: Ease + Comfort

Design can be used efficiently and comfortably, and with a minimum of fatigue, regardless of a user's body size, posture, or mobility.

### 11: Structure

The overall architecture of a user interface: the whole design should be organized in ways which help a user's mental model match the contents of the task (e.g., newspaper front page in a paper copy vs. its website). 

### 12: Constraints

Constaining the user to only allow them to perform only the correct actions (e.g, ask a user if they want to close a window if they click the button). We should stop faulty user input before it's even received. Constaints prevent the user from inputting input that wasn't going to work anyway. 

### Four of Norman's constraints

1. Physical constraints, like 3-pronged plugs, can only be inserted one way; this is a physical constraint. 

2. Cultural constraints, like facing forward on escalators or qeueuing. 

3. Semantic constraints are inherent to a situ ation: the purpose of a rearview mirror is to see behind you, so the purpose is to see behind you. 

4. Logical constraint: self-evident based on the situation at hand. For example: imagine building furniture; once you're almost done, you have 1 hole left with 1 screw. That's a logical constraint. 

[QUIZ - can you think of any times when you've encountered interfaces with constraints in them?]

### 13: Dealing with errors: Tolerance 

There are two principles that help us deal with errors if they do occur. Tolerance means that users shouldn't be at risk of choosing too much damage by mistake: supporting undo/redo is key, and the cost of mistakes should generally be minimized. First, we constrain the user and not let them make errors, but if they do, allow easy recovery.

### 14:Dealing with errors: Feedback

Feedback must be given so that users can understand why the error occured, and how it can be avoided in the future. Feedback must be informative and immediate; they should also suggest a solution. Not only should it be possible to recover from an error, but the system should tell you how to do this. 

### 15: Documentation

One goal of usable design is to avoid documentation altogether, but it can be necessary to provide this. Nowadays, docs list the tasks that you can do/may want do to rather than simply listing all the features.


## Lecture 2.6

### Mental Models

A mental model is an undersatnd of the processes, connections, and relationships in real world systems. Using these, we generate expectations/predictions about the world, and compare these to what actually happens when the event takes place. When reality doesn't match our mental model, it makes us curious, and frustrates us. As designers, we want to ensure that we create mental models that conform to people's expectations. How do we do this?

- design systems that act the way users want them to act
- design systems that teach users how to act

### Mental models and education

These aren't only important in HCI, but also, say, in education. That's relevant, because you're teaching your users how to use your UI, but your challenge is teaching users without being physically present to explain things to them. Good representations (how we achieve this) show the user to do this. 

### Mental models in action

e.g., example of Volvo and Nissan, which let users turn both AC and heat on at the same time, and don't sufficiently constrain the actions they can take.


### 5 tips: Mental models for learnable interfaces

1. Predictability - can the user predict what will happen (e.g., greying out a button helps user predict that that button won't work)

2. Synthesizability - users shouldn't merely be able to predict an action's result BEFORE they perform it, they should also be able to see the sequence of actions that led to the current state (hard in a GUI, but a log of actions in an undo menu, for example, allows us to do this).

3. Familiarity - This idea is similar to affordances. The interface should leverage actions with thiwhc a user is alrady familiar inthe interface.

4. Generalizability - knowledge of one user interface should be transferrable to others (e.g., copy/paste transferring from MS Word to Google Docs)

5. Consistency - Similar tasks/operations should behave the same way (ctrl+x should cut selected text whether text is selected or not, and not close the app if it's not selected)

### Representations

Representations help ensure people develop appropriate mental models. Using good representations is essential. IKEA instructions, for example, are unusually helpful representations of how users are supposed to build the actual furniture.

### Improving representations

- write them out
- visualize the problem
- substitute abstract items for things that have mappings to real life people/objects/animals/concepts that have similar relationships to those you're trying to explain ( squares + circles > sheep + wolves in class exampel) 

### Characteristics of a good representation

1. Make relationships explicit
2. Bring objects and relationships together (e.g., wolves and sheep)
3. Excludes extraneous details
4. Expose natural constraints (e.g., sheep and wolves)

[QUIZ: redesign a circuit breaker!]
- instead of writing representation on a paper and mapping breaker>paper, paper > objects, just write the rooms/devices on the breaker switches themselves
- map out the house on the circuit breaker so that rooms/devices are represented according to their location
- place circuit for each room in that room

### Representations in interfaces

These are incredibly important in UIs, e.g., Google calendar — white space is free time, whereas filled in blocks represent blocked chunks of time/appointments.

### Metaphors and analogies

Leveraging an analogy or a metaphor can help your users understand how your interface works very quickly. The WSJ's site, for example — the front page is a great representation of the online site, so we can use it the WSJ's print front page as an analogy. 

There's an issue, however — when you use an analogy, users don't know when an analogy ends, so you may have to be explicit about that or account for it. We need to pay special attention to the misconceptiosn that these analogies/metaphors produce.

### Design principles revisited

Using analogies/metaphors relates to consistency, since we need to be consistent with the analogies we use to explain our UI.

In the same way that we emphasize affordances, we emphasize users being able to understand how to learn what the UI does without interacting with it. 

Representations are important because they map the interface to the task at hand. It lets users predict the mapping between the interface and the actions in the world.

### New functionality, old interfaces

We always want to leverage old interface concepts with new functionality, but there's a challenge — why are we designing technology if we're not providing a wholly new thing for them to do? A thing which has no prior expectations or understanding? We'll have to, at some point, teach users something wholly new.

### Learning curves

Every interface requires users to learn something. We want a learning curve where the expertise of the user quickly reaches the adequate proficiency level without a significant amount of experience (i.e., rapid learning curve). 

How can we help users reach the requisite proficiency level faster? Generally, if we use adquate representations, as well as affordances and elements from known interfaces which users can generalize from, we can essentially raise their expertise level without any investment on their part in gathering more experience. 

Otherwise, we can focus on helping users grow their expertise as quickly as possible.

### User error: slips and mistakes 

- Slips: when the user has the right mental model, but makes a mistake anyway (e.g., when dealing with a "save your work?" prompt, if you violate conventions of order, or the default selection is wrong, a user will make a slip because your interface pushed them in a direction which violated their assumed norms).

TL;DR  - when the user knows the right thing to do but does the wrong thing anyway

- Mistakes: When the user has the wrong mental model and consequently does the wrong thing. 

### Types of slips

1. Action-based slips - where user performs the wrong action, or the wrong action on the right action on the right object, even though they knew of the right action. 

TL;DR - doing the wrong thing

2. Memory lapse - when the user forgets something that they needed to do.

TL;DR - forgetting to do the right thing.

### Types of mistakes

1. Rule-based mistakes: occur when the user correctly assesses the state of the world, but makes the wrong decision based on it (e.g., the user knowing that they want to save their document, but clickign "No" instead of clicking yes in response to a confusing prompt).

TL;DR - right knowledge, wrong rule.

2. Knowledge-based mistakes: when the user incorrectly assesses the state of the world in the first place (e.g., user didn't realize they made any changes, and hence didn't realize that they needed to save; they applied the right rule based on their knowledge, but had incorrect knowledge).


3. Memory lapse mistakes: similar to memory lapse slips, but focuses on failing to fully execute a plan, rather than forgetting to do it altogether (e.g., user shuts down computer and fails to come back to a particular prompt).


In our designs, we want to:

- prevent routine errors by leveraging consistent practices, like arrangement of menu prompts.

- let the interface offload demands on working memory from user to computer to prevent memory lapse errors

- want to leverage good representations to help users develop the right mental models to minimize rule-based and knowledge-based errors.

- we should also leverage the tolerance principle to make sure the errors don't have dramatic consequences.

### Quiz - slips vs. mistakes

[QUIZ - how can Morgan ensure she doesn't make mistakes in texting?]

### Learned helplessness

What if a user acts on an interface/task, and seems to be unable to change the output of the system using their input? That's learned helplessness: when users feel like they can't do anything with a particular interface while they use it. 

### Quiz: expert blind spot

There are parts of a task that you perform so well that you assume they're obvious, or forget to say when teaching them to someone else — that's an expert blind spot. Since you're teaching a user to use the interface that you've designed when creating UIs, you need to remember that they won't know many of the things you take for granted. 

[QUIZ: name an instance of learned helplessnes, and an instance of expert blind spots]

### Types of mistakes

1. We first discussed mental models (what they are, and how users use them to make sense of a system)
2. Then we talked about how good representations help users achieve strong models
3. We then talked about hwo issues with interfaces can lead to user error: slips and mistakes
4. We then talked about learned helplessness, and bad feedback on user errors, as well as expert blind spots


## Lecture 3.5

### Basics of prototypes

Early prototypes:
- rapid revisions of preliminary ideas

Late prototyping
- putting finishing touches on things

First variable that early vs. late prototypes differ on is representation:
From early to late (and from easiest to hardest, from least time-consuming to most time-consuming):

- verbal
- paper
- wizard of oz
- wireframe
- physical
- functional
- live 

Second variable: Fidelity

Low fidelity: paper, simple drawings, easy to change. 
    - used for evaluating function of an interface. 

High-fidelity: built out, functional and engineered prototypes
    - used for evaluating performance. 

Third variable: Evaluation

- Function: Low fidelity variables evalute the scope of an itnerface. We evaluate whether the prototype has the potential to do what we want it to do, like whether the user can read something and understand it.

- Performance: We evaluate specific tasks through higher-fidelity prototypes.

Fourth variable: Scope. 

Is it a horizontal, or a vertical, prototype?

- Horizontal: cover the whole design, but in a shallow, broader way (e.g., whole site)
- Vertical: take a very specific aspect and prototype it very thoroughly (e.g., specific site function)

We generally start with horizontal prototypes, and move to vertical ones eventually.

### Tradeoffs in prototyping

Low-fidelity: easy to create/modify, but aren't effective for actual evaluations
Hi-fidelity: hard to put together, but effective for actual evaluations

Note that throughout this whole process, we aim to get lots of feedback.

### 5 tips for effecitve prototyping

1. Keep prototypes easy to change: the goal is rapid revision and improvement
2. Make it CLEAR that it's a prototype: you don't want users to focus on superficial elements
3. Be creative: the goal is to get feedback, so don't let the prototypes limit the feedback you receive. If you're not getting the right feedback for your
prototype, you can create new ones.
4. Evaluate risks: one of the biggest goals of prototyping is to the time spent pursuing bad design by getting feedback on them early. Try to get feedback to make sure you're not wasting time, and show the featurs that might not work for users in your prototype to save yourself any future work!
5. Prototype for feedback: The goal of a prototype is to get feedback, so prototype for the type of feedack you want to get. 

### Verbal prototypes

The lowest fidelity prototype; v. easy. 

Social desirability bias is an issue, so we have to make sure to ask in a specific and critical way which encourages users to disagree if they don't like the idea.

We also have to make sure that the user undertands what we're talkign about, and that we're not falling prey to expert bias + assuming that our explanation makes sense to a novice. Analogies can be powerful tools to control for this bias. 

### Paper prototyping

This is the next level of prototyping, and refers to sketching out the interface. It's pretty lo-fi, so users know that they need to focus on things like layout and elements.

You can also do some interaction with this, by drawing out the various other screen possibilities. This is also called "card prototyping" by putting different screens on different cards.

### Wizard of Oz

Paper prototyping works well if you're prototyping a screen, but how do we prototype voice or gesture interfaces? Wizard of Oz prototyping, which refers 
to doing the things that the interface would do once it's implemented, "behind the screen" as it were. We simulate the behavior of the interface.

These can get pretty complex, but in concept, it's a way of havin ghte user imnteract with a sytem while a human supplies the functionality that's yet to be implemented.

### Wireframing

Incorporating screen real-estate issues, font sizes/colors, etc., which give users a better sense of what the end product will look like. 

### Physical prototypes

Wireframing's great for on-screen interfaces, but what about a physical interface? You can build one (but it doesn't have to work)! We can get feedback on whether merely holding a similar device in the same context as the task is inconvenient, and perhaps, if there are buttons that match those of the final itnerface, incorportate the user's ability to press them into our testing, all without the prototype actually working.

### Design lifecycle revisited

We may think that we move on to evaluation once we're done prototyping; we don't jump from paper prototypes to wireframes to actual products! After each prototype, we evaluate what we've created, follow through to another needfinding exercise, and then move through to another design alternative, which we prototype. This cycle can take as little as one minute, throughout the context of a conversation with a user, or months, if we're developing the functional, hi-fi prototype.

### Multi-level prototyping

All prototypes don't have to be at the same level, at the same time. Different parts of the interface can and should exist at different levels of fidelity, depending on what youre' workign on. 

## Lecture 2.7: Task analysis
The task is always at the heart of the exercise. We'll discuss 2 methods for articulating the tasks that people are completing. 

1. Human information processing model, esp. the GOMS model, which focuses on input to the user and output from the user. This is similar to the processor model
2. Cognitive task analysis, which involves getting into the user's head (similar to the predictor model of the user)

### GOMS model
It builds of the processor model of the human's role in the system. It gets its name from the 4 sets of information it focuses on gathering about a task:

- Goals
- Operators that the user can perform in the system
- Methods the user can use to achieve those goals 
- Selection rules that a user users to choose among different competing methods. 

This model proposes that every human interacting with the system has a goal that they'd like to accomplish, a set of methods they can use to accomplish those goal (each of those methods is comprised of a series of operators that carries out that method), and they have some selection rules that help them decide which method to use. 

### GOMS model in action

1. We have an initial situation, where we need to transfer information to a coworker; this carries the implicit goal of the information having been transferred.
2. We may have a number of methods in mind for doing this: we can email, type a chat message, walk there in person, call them by phone, or text them by phone.
3. We have some selection rules for how we choose among those methods (e.g., time sensitive? Information complex and detailed? Casual?)
4. We then execute the operators that these methods consist of.

e.g., using the GOMS model to describe the task of shutting off a security system

Goal: shutting off security system
Methods: Using a keypad and use keychain 
Operators: the individual activity steps needed to perform each of these methods

### Strengths and weaknesses of GOMS model
Weaknesses:
- doesn't address complexity of the problems
- assumes the user already has these methods in mind; don't do a good job of accounting for novices or user errors (e.g., you may not know what the methods are, let alone the best way in which to choose among them)

Strengths:
- Ability to formalize user interaction into steps that we can use to make predictions. We can measure how long each of the operators takes, and can thus predict the efficiency.

### GOMS family

There are 4 popular variations of the GOMS model:

KLM-GOMS: Keystroke level model
The designer specifies the operators and executions times, and sums them to find the complexity of the interaction. 

CMN-GOMS
Extension of GOMS that features submethods and conditions in a strict goal hierarchy. There is a high level of granularity in the goals here. The authors use this to model points where there is a lot of complexity that can be cut out, and looking at the number of interactions required to see if this can be reduced. 

Natural GOMS Language:
A natural language form of GOMS that lends itself to human interpretation. In this model, we make the assumptions, actions, and operators more detailed.

### 5 tips for developing GOMS models

1. Focus on small goals: e.g., navigating to the end of a document. Smaller, moment-to-moment goals should be the focus.

2. Nest goals, not operators: operators are the smalles atoms of a GOMS model. For example, while the overall goal may be "driving home," the sub-goal is "changing langes."

3. Differentiate descriptive and perscriptive - identify whether you're building a model of what people actually do, or what you think they'll do. 

4. Assign costs to operators - GOMS was designed to help us make predictions about how long certain methods will take. The only way to do this is if we have measurements of how long certain operations take (usually this is time, but depending on the domain, we may be intersted in phrasing hte cost differently too).

5. Use GOMS to trim waste - GOMS lets us visualize where an unnecessary number of operators is required to accomplish some task. This is bolstered by the costs assigned to those operators. Use GOMS to find out whetere the operators required can be simplified by the interface.

### GOMS to Cognitive task analysis

Cognitive task analysis examines tasks, but puts a higher emphasis on memory, attention, and cognitive load.

### Cognitive task analysis 
This isn't a single method, but a type of method for approach the evaluation of how people complete taks. We're interested in what kind of information we're trying to gather, not how we're gathering it. These analyses are concerned with the thought process involved in performing a task. Most methods follow a common sequence:

1. Collect preliminary knowledge: we might observe people performing a task (e.g., for navigation, watch someone using a GPS)

2. Identify knowledge representations: what kinds of things does a user need to know to complete a task? We want to know not the *actual* knowledge that they have, but the *types* or structures of the knowledge. Does this task require a series of steps that must be done in a certain order? Does it require a collection to check off in any order? Does it require a web of knowledge to memorize? e.g., for navigation, the structure of the knowledge is a number of things in order, as well as a number of things to monitor as we go.

3. Apply focused knowledge elicitation methods - We want to populate these knowledge representations. Here's where we find out what the user actually knows. e.g., for navigation: start GPS, enter address, obey turns while monitoring traffic and speed. During this speed, we identify 
 - the specific actions they take
 - the knowledge they must have in mind to take those actions
 - the interruptions that can change their thought processes
 - the equipment involved
 - the sensory experience of the user

To do so, we get users to tell us what's happening in their heads and their environment. Sometimes we do things to help us understand parts of the tasks that the user isn't even aware of. 

4. Analyze and verify data acquired: part of this is just confirming that our understanding of what we observe is correct according to the subjects of our observations (present result to users and make sure they agree with our understanding). Then we formalize this into structures which can be compared and summarized across multiple data gathering methods. 

5. Format results for the intended application: we want to develop models that show what the user was feeling, thinking, and remembering at any given time and make those relationships explicit.

### Hierarchical Task Analysis 

Large tasks are often made up of smaller tasks. When we discuss driving, for example, tasks like "monitoring route" and "monitoring traffic" are so high level that it's almost useless to describe driving in these terms. Each part can be broken out into smaller subtasks; additionally, these smaller tasks can be used in multiple contexts. Route monitoring, for example, can also be used while riding as a passenger or runnign. The analysis of a task in a particular context can be useful when designing interfaces for another context, if we break the analysis into subtasks. 

e.g., Buying something online:

1. Fill cart
2. Proceed to checkout
3. Enter shipping info
4. Enter billing info
5. Enter card info
6. Review purchas
7. Complete purchase

Most of the tasks involved here are general to anyone shopping on any website, but every single website has to create this functionality. If we start to treat this cognitive task analysis more hierarchically, we can start to see a well-defined subtask around the checkout process:

1. Fill card
2. Proceed to checkout
3. Checkout
    3.1 Enter billing info
    3.2 Enter Card info
    3.3 Review purchase
4. Review purchase
5. Complete purchase

Every online vendor has the checkout steps in their checkout process, so we can potentially incorporate existing payment widgets/technologies like paypal to make those steps smoother. This hierarchical task analysis helps us understand which tools are already available to help us accomplish certain portions of our task, or how we might design certain things to transfer between different tasks and different contexts. 

Hierarchical task analysis also helps designers abstract over a part of the site that can be added in via turnkey methods and focus on parts that are more unique.

Strenghts of using hierarchy:
- abstracting out unnecessary details for a certain level of abstraction
- modularizing designs or principles so that they can be transferred between different tasks or contexts
- organizing the CTA in a way that makes it easier to understand and reason over. 
- Cognitive and hierarc

### CTA strengths and weaknesses

#### Strengths
- Emphasizes mental processes: unlike the GOMS model, CTA puts an emphasis on what goes on inside a user's head. Consequently, it's better for assessing how experts think and work.
- The information it generates is formal enough to be used to be used for interface design, for comparison among alternatives, etc.

#### Weaknesses
- There are disadvantages, though — these are incredibly time-consuming to perform. They involve talking to multiple experts for long periods of time and systematically analyzing the data.
- These risk de-emphasizing the context. In zooming in on the individual's own thought processes, CTA risks de-emphasizing details that are out in the world, like physical abilities, or interactions among other people out in the world.
- Isn't very well suited to novices. It's suited to experts that have very strong models of the way that they work and clearly understand their own mental thought processes, but not for novice users who are still trying to learn how to use an interface. 

### Other task analysis frameworks. 

Human information processor models:
- KLKM 
- TLM
- MHP
- CPM-GOMS
- NGOMSL

Cognitive models:
- CDM, Critical decision method: puts a focus on mplaces where critical decisions occur.
- TKS, Task-knowledge structures: focuses on the nature of humans' knowledge. 
- CFM, cognitive function model: focuses on complexity
- Applied CTA, skill-based CTA: two ways of gathering information necessary to create cognitive model. 

There are other frameworks more common in other disciplines, such as production systems, which are common to AI. 


## Lecture 2.8 - Distributed Cognition

This pertains to the context in which HCI takes place. We'll look at 4 different models of the context surrounding HCI, and focus primarily on distributed cognition, which refers to the dominant theory surrounding multiple agents, artifacts, and contexts. We'll also touch on social cognition, situated action, and activity theory.

### Distributed cognition

Cognition is interested in thought processes and experiences, but distributed cognition suggets that models of cognition should be distributed outside the mind. 

e.g., take a hard addition problem; we can't remember numbers given to us, but we can write them down, and do the calculations by hand. We don't get smarter when we grab a pen and paper, but the system comprising the agent, the pen, and the paper is more powerful than the system which involves the individual on their own. The cognition, in this case, is distributed among these artifacts: the paper remembered the numbers for the agent, and tracking the arithmetic progress as the agent worked. 

### Paper spotlight: how a cockpit remembers its speeds

The cockpit is more that just the cage - it's the whole system, which includes the person — that remembers the speed, as a whole, rather than the pilot alone. 

For every flight, pilots need to adjust the wings at various speeds; because pilots must act quickly, there's a high cognitive load. How do we address this? 

The cockpit contains a booklet which indicates the various speeds at which different parameters should be adjusted; that's the LTM. Prior to the descent, the pilots find the relevant page, and pin it in inside the cockpit. This becomes a form of the cockpit's short-term memory (we attribute this to the cockpit's memory, because the pilots themselves don't remember this). As pilots begin the descent, they mark the speedometer with speed bugs to help them remember when they need to make a change. This is like the working memory, because pilots only visually compare, rather than access short-term memory. No single part of this cockpit could perform the necessary actions for landing a plane on their own. 

### Distributed cognition and cognitive load

These two concepts are closely linked, because artifacts allow you to get additional cognitive resources, or share the cognitive load across multiple resources. 

### Quiz: distributed cognition for paying bills:

Note that we're specifically interested in artifacts sharing the COGNITIVE load. Remember that equipment that's within the system but doesn't serve this purpose doesn't help in distributed cognition. 

### Distributed cognition as a lens

This isn't a design principle — many of our design principles are great examples of distributed cognition — but rather, a lens to view design. A calendar, for example, is an example of distributed cognition. It helps us think of what systems can accomplish as opposed to individuals on their own.

### Distributed cognition to social cognition

Distributed cognition focuses on how the mind can be extended by relations with other artifacts and individuals. Distribution among individuals used to be much more important, than it is today, and than we generally focus on as HCI designers. Before GPS navigation existed, for example, it was your spouse or your friend that would sit next to you and read a map to you. JIRA, for example, is a system that comprises the people that perform the tasks, and the JIRA tool itself.

### Social cognition

The social portion of distributed cognition is concerned with how social connections create systems that can, together, accomplish tasks (e.g., driver and navigator). It's also concerned with the cognitive underpinnings with social interactions themselves. Why do we care? Because social media is a Big Thing these days. Oftentimes, however, our interfaces are at odds with our desires — our friends may not want to know how much we play video games. 

### Quiz: how do we design a social video game system that protects certain behaviours from becoming public and interfering with our social tasks?

Specifically, we probably don't want video games to tell people how much we spend time playing them. We can base social video game relationships on only mutual relationships — your behaviors are only seen by those who share them.

### Situated action

This is strongly concerned with the context in which people interact; unlike distributed cognition, it's not interested in the long-term, enduring interactions among these things. It's not that it denies the existence of long-term memory, but has a differnt focus. It's interested not in problems that people have solved before, but in the kinds of novel situational problems that arise all the time. 

e.g., David is filming the lecture with his baby daughter; he doesn't know how it's going to impact his performance as a lecturer. This is the kind of interaction that situated action is interested in. While we like to think that we're in charge of structuring a task for our users, in reality, the tasks that we perform are growing out of this interaction. Once we've got out hands on it, the task is what we do, not what we design. 

There are 3 key takeaways:

1. We must examine the interfaces we design withinthe context in which they're used
2. We must understand the task that that the users perform grows out of the interaction with the interface. We don't define it. 
3. We can try to structure it as much as we can, but until the users get started, the task doesn't exist. Once they get started, they play a significant role in defining the task.

### Situated action and memory

Recognition is easier than recall, partly because memory is so context-dependent, and relying on recall means there's little context to cue the person's memory.

e.g., David's mother needed help with a few things every time he saw her, and although he would try to help, he would forget something each time. The reason he forgot and she didn't was because for him, the tasks were items on a list; for her, meanwhile, they were part of a larger context. 

### Paper spotlight - Plans and situated actions: The problem of human machine communication

The book is a comparison of two views of human action. The first view views the organization + significance of action as derived from plans. This is the model we frequently use when developing interfaces — users make plans, and carry out those plans. In the second view, people simply act in the world, and plans are what we derive from those actions. Instead of plans dictating actions, plans are our interpretations of those actions. This means that rather than assuming that a user has a plan in mind that they're carrying out, we may  consider their current actions with the screen instead. Forget their history, and ask, "once they're here, how do they know what to do next?"

### Activity theory

        Activity
        ↓       ↓   
    action      action
    ↓    ↓           ↓     
operator operator    operator

This is one of the first places the idea of "interacting through" an interface came from, and predates HCI. In our conversations about HCI, there are 3 main contributions of activity theory:

1. When we discuss designing tasks, and completing them through an interface, we risk missing a key component: WHY is the user completing the task in the first place? Activity theory generalizes our unit of analysis from the task to the activity. We're not just interested in what they're doing by WHY they're doing it, and what it means to them. Our designs will be different, if, say, users are required to, vs. because they choose to. In distributed cognition, the unit of analysis was generalized from a person, to a system with people and artifacts. Here, we're generalizing the unit of analysis from a task to an activity surrounding a task. We're zooming out from a task and design space. 

2. Activity theory puts emphasis on the idea that we can create low-level operations from high-level actions (similar to GOMS models, where methods are made up of operators). Before activity theory reached HCI in the 80s, HCI was concerned with minute things like how quickly a person can click a button or type in a command. Activity theory helped us zoom out from low-level operators to higher level needs in the action or activity levels. 

3. Activity theory points out that actions by the user can move up and down the hierarchy we outlined at the beginning of the activity theory section. 

e.g., driving a car: the first time shifting gear between "park" and "drive" was a very conscious decision. You would think about how to do it, and which way to push the stick. After a while, however, shifting gears becomes second nature; it shifts from being a conscious goal to being a mere operator in your broader driving behavior. 

There's a lot of similarity to our previous discussion about learning curves: how quickly an action goes from being a conscious action to a subsconsious operator is also a function of how good the learning curve is on a particular design. It's similar to the question of invisible interface: a good invisible interface helps users focus on the *actions* inside the task rather than the *operators* they need to use to interact with the system. 

### Paper spotlight: Activity theory and HCI

Activity theory offers a set of perspectives on human activity, and a set of concepts for describing that activity. That's exactly what HCI research needs to describe context, situation, and practice (written in 1996). 

### Paper spotlight - Studying context: activity theory, situated action, and distributed cognition.

Activity theory and distributed cognition are driven by goals; meanwhile, situated action deemphasizes goals and focuses on improvisation. She summarizes situated action by saying that goals are constructed retroactively, to interpret our past actions. 

She also evaluates the roal of permanent, persitent structures, noting that they're important for activity theory and distributed cognition, but present a tension for situated action. 

What makes them different? Main difference between activity theory and distributed cognition is their evaluation of the symmetry between people and artifacts:
- activity theory emphasizes the importance of motives and consciousness, which belong only to humans, sees artifacts and people as different.
- distributed cognition believes that artifacts can serve cognitive roles, so can be considered conceptually equivalent to humans. 

## Lecture 3.6 - Evaluating prototypes
Just as different prototypes have different functions at different moments in the design process, so also is the case with our methods of evaluation. Early on, we want more *qualitative* feedback: what people like/don't like, whether it's agreeable, whether it's understandable. Later on, we want to know if it's usable: is it inuitive? Is it easy to learn? 

At the end, we may want to evaluate things in a more *quantitative*, *empirical* way. We may wanna measure whether the time to complete a task has changed, or whether the nubmer of sales has increased. 

Along the way, we may want to iterate even more quickly by predicting what the results of the user evaluation will be through *predictive evaluation.* 

The type of evaluation we employ depends largely on where we are in our design process. 

### Evaluating interfaces
 
There are three categories of this:

1. Qualitative - where we get qualitative feedback from users, which emphasizes the _totality of a phenomenon_. We'll get this data through methods identical to the ones we used during the needfinding stage. 

2. Empirical - based on numeric summaries or observations of a phenomenon - where we run a number of controlled experiments and evaluate their results quantitatively. For this, we need many more participants, and want to make sure that we've addressed the main qualitative feedack first. 

3. Predictive - based on systematic application of pre-established principles and heuristics. This specifically refers to evaluation without users. Because evaluation with real users is slow and expensive, we occasionally opt for this option (although it's not our preferred choice, because we emphasized user-centered design)

### Vocabulary used in evaluation

- reliability: whether measure consistently returns the same results for the same phenomenon across time
- validity: whether a measure's results actually reflect the underlying phenomenon. 
- generalizability: whether measure's reuslts can be used to predict phenomena beyond specifically what's measured. Can we apply lessons from our evaluations to broader groups of people?
- precision: the level of detail a measure supplies. How specific is it?

### 5 tips for what to evaluate
1. Efficiency: how long does it take users to accomplish certain tasks (e.g., fewer actions, less time)?

2. Accuracy: how many errors do users commit while accomplishing a task? Ideally, we want an interface that reduces the number of errors that a user commits while performing a task. 

Both efficiency and accuracy, however, measure the performance of an expert user using an interface. 

3. Learnability: Sit a user down in front of an interface and define a minimum level of expertise; how long does it take them to reach that floor of expertise? Expertise can be broad (creating a full text document with formatting) or narrow (cutting and pasting text).

4. Memorability: refers to the user's ability to remember how to use an interface over time. 

5. Satisfaction: Users' enjoyment of the system, or the cognitive load they experience as a result of using it. To avoid social desirability bias, we may want to test this in unexpected ways, such as monitoring how many participants downloaded the app they tried after their test session was over. 


It's important to articulate, at the outset, 
- what you're evaluating
- which data you're gathering 
- which analysis you'll use

These 3 should match up to address the research question.

### Evaluation timelines: purposes and methods

Much like with prototyping, the methods we use for evaluation change from the beginning to the end of the design process. 

_Purpose_
Throughout most of the process, evaluations are *formative*: that is to say, they help us redesign and inform oour interface, and help us improve the interface going forward. 

At the end, however, they are *summative*: they help us conclusively evaluate the impact of our interface on the task. 

_Methods for fulfilling these purposes_
Consequently, our early evaluations tend to be more qualitative. Their goal is to help us to improve/understand a task. 

In later evaluations (the empirical ones), the goal is to assess change. Predictive evaluations tend to be similar to qualitative evaluations. 

_Evaluation data types_
Similarly, qualitative data tends to be collected earlier, and quantitative data later. In reality, qualitative data is *always* useful to us, but quantitative data is only of use to us when we're exceedingly rigorous abouts its collection + quality + evaluations.

_Setting_
Finally, does the evaluation take place in a controlled lab setting? When we're testing low-fidelity interfaces, we generally want to do it in the lab (consequently, this happens earlier). We want to describe the rationale behind certain decisions to participants, and have them in a less natural setting to get their feedback. Later on, we want to give users a working prototype, so that they use it in the context of their everyday lives. 

While these are loose guidelines, we generally want to stick, at least, to the order of these steps.

### Evaluation design - how do we do it?

1. Define the task: depending on the moment in the design process, this can be very large, or quite limited (e.g., when designing Facebook, do we want to evaluate the way that a UI helps people post an update, or navigating between and using multiple pages?). We want to *clearly* define which task we're going to investigate.

2. Define our performance measures: How are we going to evaluate the user's performance? Qualitatively, it can be the written or spoken feedback about their experience. Quantitatively, we can measure efficiency in certain activities or count their accuracy. Defining performance measures a prior helps prevent confirmation bias because we can't pick and choose the measures that we retroactively value as important. It forces us to be objective.

3. Develop the experiment. How will we collect users' performance on our measures? Will we have them think aloud while they're using the tool, or take a survey while they're done? What will we measure, what will we control, and what will we vary? Are our assessment measures reliable and valid? Are our findings generalizable considering the user group we're examining?

4. Recruit participants. We need to recruit participants who are aware of their rights and are participating willingly.

5. Do the experiment.

6. Analyze the data. We need to be impartial, so that if we find evidence supporting the idea that our interface is superior on measures other than the ones we had outlined initially, we should run a follow-up experiment rather than simply jump on the supporting data.

7. Summarize the data. We do so in a way which informs our ongoing design process: what's works? What can be improved? How can we take the results of the experiment and revise our interface to its benefit?

The results of this experiment feed into our design life cycle. After needfinding and coming up with design alternatives, we prototype; we then evaluate the prototype, and use the results to refine our understanding of user's needs, before producing more design alternatives.

### Qualitative evaluation

Example questions:
- what did you like/dislike?
- what were you thinking while using this interface?
- what was your goal when you took that particular action?

The methods we use for qualitative evaluation are very similar to the methods we use for need-finding: interviews, think-aloud protocols, focus groups, surveys, post-event protocols — we use these methods to get information about tasks in the first place, and now can use these same methods to get data about how this protocol changes the task.

### Desigining a qualitative evaluation

Some questions we'll have to answer when we design a qualitative evaluation:

1. Is this based on prior experience, OR is it a live demonstration?

If you're bringing in users to answer questions about an interface they're already using regularly, this is actually an instance of needfinding rather than something else. Meanwhile, in the context of evaluations, you're generally bringing users in to test a new interface prototype.

2. Is the session going to be synchronous OR asynchronous?

In a synchronous session, you're watching them use the prototype live. If they complete it on their own and then send you the result, the session is asynchronous. Synchronous is usually beneficial, because we see a greater amount of the interactions taking place. We may also be able to interrupt the user and get their thoughts, live. Asynchronous, however, is much easier to carry out, esp. with larger populations. Synchronous is generally better, but asynchronous ends up often being the only option.

3. How many prototypes will users be evaluating? One OR many?

If you've got many, you should vary the order in which you present them, so that you don't get consistently get feedback because the user's already familiar with the problem domain in which the interface functions. This can, especially, be an issue when you're comparing new interfaces vs. an interfaces people have used in the past. 

4. When do you want to get feedback from the user — are you using a think aloud protocol OR a post-event protocol?

In a *think aloud* protocol, we ask the users to talk to us while they're using the interface. They explain what they're seeing, how they interpret it, and what they think the outcome of their actions will be. 

In a *post-event* protocol, users go through some session using the interface, and then giving you the thoughts at the end. 

Both of these have an important drawback: users are often unable to articulat precisely why they like/dislike some feature. Additionally, the drawback of a post-event protocol is the fact that users only give feedback at the end, so if they experience difficulties early on in in the process, they may forget it by the time you get feedback from them. The drawback of a think aloud protocol, meanwhile, is that it may introduce biases: when users think aloud when using the interface, the way the use the interface actually changes. They're more deliberative and thoughtful. This means that that while users may figure out how to use an interface using a think aloud protocol because they're being more thoughtful, when they use the interface on their own, in a natural manner, they may be unable to figure it out. Thus, it's good to use a mix of these two protocols: Prof. Joyner recommends starting with a think aloud protocol, and using a post-event protocol as more of a summative evaluation once you're confident. 

5. Do you want feedback from individuals or groups?

Focus groups are used when multiple users talk about their experiences, and can lead to better explanations because users build on each other and expand on one another's ideas. It can, however, bias the group towards the opinions of the most powerful personalities, unlike individual interviews and surveys, where people feel free to speak as they wish. 

### Capturing qualitative evaluation

We want to capture as much of the session as possible because in qualitative research the totality of the session. How do we do this?

1. Video recording

_Pros:_

- automated - runs automatically during the background
- comprehensive - records everything
- passive - lets us focus on administering the session instead of capturing it

_Cons:_
- intrusive - people are uncomfortable with this and the amount of data it captures
- difficult to analyze - super hard to code this as a rater
- screenless - hard to capture interactions on screen.

Some of these issues can be resolved, for example, using a screen recording and syncing it up to a video camera's recording of a participant. If we're dealing with delicate subject matter or populations that may not be comfortable being in front of a camera, this method may not be ideal. It also takes FOREVER to go through this video footage.

2. Note-taking

_Pros:_
- Cheap
- non-intrusive - only captures what we decide to capture, and we dont' have to capture every foible of a person's behavior
- easy to analyze (vs. video)

_Cons:_
- can be a slow process and hard to keep up with what's happening
- manual - gets in the way of adminstering the session (to do this well, you'd want to have 1 person taking notes, and another administering the session)
- limited - may not capture some of the motions/movements of a person when interacting with an interface, or the duration of their hesitations before deciding what to do next. 

3. Software - log the behavior using software


_Pros:_
- automatic
- passive
- analyzable because it's in a data/text format

_Cons:_
- limited - only captures the things that are expressed inside the software
- narrow slice of the interaction is captured
- tech-sensitive - need to have a working prototype for this to be an option

### 5 tips for qualitative evaluations

1. Run pilot studies: you want to make sure that once you're ready to have real users involved, you're ready to experiment with real data. Run the pilot on friends, co-workers, or family to iron out any kinks in your evaluation design.

2. Focus on feedback: it's tempting to focus on teaching a single user the rationale behind certain features, but remember, use this as a tool to figure out user needs broadly, rather than educate a single user about specifics of the interface.

3. Use questions when users get stuck. That way, you get information on why they're stuck and what they're thinking. That way, you can also use questions to guide users to how they should use the interface to let the session seem less instructional.

4. Tell users what to do, but not HOW to do it. We want users to be able to user interfaces without specific instructions. If they try to do so differently to the way you expect, you can use this behavior to indicate how you should design the next interation of your interface.

5. Capture satisfaction. Soemtimes we get so caught up in whether or not users can use our interface that we forget to ask them whether or not they like using it. Make sure to capture this in your evaluation!

### Empirical evaluation

We're trying to evaluate something formal, or numeric. There could also be interpretation involved, like summarizing survey results or counting errors. The ultimate goal, however, is to come to something verifiable and conclusive.

In industry, we use this to compare designs to one another. In research, this is even more important, because we use it to create new theories of how people think when they're using interfaces. If we want to, say, prove, that gestural interaction is harder than voice interaction, we'd need to empirically evaluate these claims. 

Most empirical evaluations are comparisons, because the benefit of empiritcal data is that it can help us perform objective comparisons. The empirical question, therefore, is "how can we show the differences between these designs?"

### Desigining empirical evaluations

We have multiple conditions, called treatments, which refer to what a participant does in an experiment. These could be different designs, different colors, or whatever we're interested in investigating. 

We have to be careful that the differences we observer really are due to the differences between treatments and not other factors. e.g., which logo color is better? If one is also a triangle while the other is a circle, we can't say if we compare the two, because we're not controlling for logo shape. 

Once we've designed the treatments, it's time to design the experiment itself. The first thing we ask is: what do the participants do? Does each one participate in one treatment, or both? 

If participants participate in a single treatment, we split our sample into 2 groups, at random, and they go through each treatment. At the end, we compare the dat aof the two groups. This is referred to as a *between subjects design*, becuase we compare the results of two different groups, which go through different treatments.

We can also run a *within-subjects design* for our experiment, where each participant experiences all the treatments. We have to assign participants to the order they'll receive the treatments in, to control for the effects of order to performance. This is a good design if the sample size is limited. It also lets us hone in on how certain subjects were affected and differed, like having different strenght when performing a task (we can't do this with between-subjects designs). The downside of this is that this design takes up more of our subjects' time.

We should make a special note of the importance of random assignment: this helps control for any hidden confounds, such as the possibility of your getting better at administering the procedure.

### Hypothesis testing

Example: reaction time study - which color should we use to tell a driver in a car that they're starting to leave their lane?

We run half of the experiment with participants in the orange condition, and half with green. Let's compare these two groups' reaction times. How can we compare the performance? To test this, we use hypothesis testing. Initially, we make a null hypothesis that these two groups are the same. If there's a lower than 5% chance that the difference between 2 groups arose because of the treatments, we reject the null hypothesis.

### Quantitative data and empirical tests

The kinds of hypothesis testing depends on the type of data we have.

1. Nominal:

- chi-squared test, which tests if the distribution of values in a number of buckets is the same across two conditions.
- also: Fisher's exact test, and G-test

2. Ordinal:
- Kolmogorov-Smirnov test: similar to the chi-squared test, but it's sensitive to the fact that the categories we're dealing with are ordered
- also: chi-squared test, median test

3. Interval + Ratio:


- Student's t-test: lets us compare the means of two categories and see if the differences are statistically significant. We only use this when the data is expected to be normal.
- also: Mann-Whitney test, Kruskal-Wallis test (we use these when the base distribution isn't normal)

### Special statistical tests

There are multiple assumptions that we've had. So far, we've only discussed having 2 levels of our individual variables. What if we have >2? You may be tempted to attempt 2x pairwise comparisons, but that would introduce the likelihood of a *type 1 error* (false positive), which entails the false rejection of a null hypothesis. 

For *nominal and ordinal data*, we can use the same chi-squared test, but it doesn't tell us _where_ the difference lies (i.e., between which tests). Generally, we do a chi-squared test on all the levels, and if that's significant, we do a pairwise comparison between 2 conditions, pair-by-pair. 

For interval and ratio tests, we use a different test, called an Analaysis of Variance (ANOVA). A one-way ANOVA lets us compare between multiple groups simultaneously. With a 2-way ANOVA, we can have 2 dimensions (i.e., 2 IVs with multiple levels), but this requires high sample sizes to be meaningful. It will tell us _if_ there are differences, but not where those differences lie. 

In these cases, we've always assumed that IVs have been categories. In some cases, IVs _aren't_ categories. Imagine if they're interval or ratio data; e.g., let's say we're testing GPA as a predictor of course performance. Generally, we'd be doing a linear regression (alternatively, logistic regression, polynomial regression, etc.)

A final type of data: *binomial data*: data with only 2 types of outcomes, such as success/failure, which doesn't have a standard deviation (hence can't use t-tests). We can use a two-sample binomial test if we have 2 sets of trials, or a one-sample binomial test if we already have a bedrock ratio that we're comparing things to. 

### 5 tips for empirical evaluations

1. Control what you can, document what you can't
2. Limit your variables - noisy, difficult data stems from varying multiple variables. You'll have better results
3. Work backwards in planning the experiment - figure out which question you'll want to answer, then the analysis you'll need to use, and then the data you'll need to gather
4. Script your analyses in advance - Nobel Laureate in econ Ronald Coase once said "if you torture the data long enough, nature will always confess." This means that if we analyze the data enough times, we can always find conclusions, but that doesn't mean they're actually there. 
5. Pay attention to power - power refers to the size of the difference that a test can detect, and this is highly dependent on the number of participants you've got in your experiment. Large samples > detect small effects, high power. Small samples > detect large effects only, low power.


### Predictive evaluation

This refers to evalutions you can do without actual users. It should only be used when we wouldn't be doing evaluations at all. 

### Types of predictive evaluations: heuristic, model-based, simulation-based

The first way is *heuristic evaluation*: we hand off our interface to some expert or another, and they comment on whether it violates some heuristic or another that's accepted in the canon (i.e., our 15 principles of good design). 

In *model-based evaluations*, we use models, and trace through the context of the interfcace that we designed. Just as we computed a GOMS model for what users did in some context, we can use it to create a model of what they'll do in our interface, and evaluate the efficiency of our model. We can also use the user profiles we developed to assess whether the model applies to users in each profile.

We can also take the model-based evaluation to an extreme and transitio to a *simulation-bsed evaluation*. This entails contructing an AI to automate feasibility evaluation, which is a huge undertaking. 

### Types of predictive evaluations: Cognitive walkthroughs

The cognitive walkthrough is the most common type of evaluation, wherein we step through the process of interacting with an interface, and mentally simulating what a user is seeing, thinking, and doing. 

e.g., Leaving a note while listening to an audio book. Let's pretend to be a novice user. Will they know what to do? If there's a button that says "Make note" they likely will. At every stage in the process, we want to investigate things from the perspectve of the gulfs of execution and evaluation. Is it reasonable to expect the user will be able to cross the gulf of execution? Is the right action sufficiently obvious? Is the respose the one that the user would expect? Is it reasonable that the feedback will cross the gulf of evaluation? 

There's a drawback: we're the designers of the sytem, so we automatically think that everything works perfectly; still we need to try to put ourselves in the user's shoes, which will help us uncover some takeaways.

### Evaluating prototypes
Our goal is to apply multiple evaluation techniques to continuously center our design around the user. Evaluation, because of this, is key to user-centered design.

### Conclusion
We've discussed the basics of evaluation: when to use qualitative, empirical, and predictive evaluation types.


## Lecture 2.9

We'll focus on :

1. Designing for change
2. Anticipating the change from our designs

We'll also talk about "Value-sensitive design"

### Change: a third motivation in HCI

The three motivations are:

1. Help a user do a task: Most of the time we're interested in designing for usability in HCI. 

2. Understand how a user does a task: At other times, we're interesterd in designing for research. 

3.Change how a user does a task in line with some value we hold: A third motivation is changing user behavior in line with some value that we have — this can actually conflict with the prior motivation of usability, such as cars that cap your speed so that you don't get hurt. 

### Paper spotlight: Do artifacts have politics?

Two distinct ways in which artifacts can be political:

1. Inherently political technologies: only compatible with certain political structures, like nuclear power, because it needs a certain amount of top-down organization. 

2. Technical arrangements as forms of order: technologies can be used to change the social order when used in a particular way. For example, a Chicago factory introduced robots to take human jobs as a way to bust up the union. The new technology was inferior, and the quality of the products dropped, but it was used to serve a political purpose. 

Artififacts, then can either inherently be political, in that they're only compatible with certain forms of political order, or they may be used to achieve political motives even though they may not have politics as an inherent part of themselves.

### Negative change by design

There are many instances where people create ostensible 'neutral' designs but that nevertheless have political motivations behind them. Winner describes the case of Bob Moses, who planned NYC in 1900s. He oversaw multiple parks in Long Island, as well as multiple parkways, which brought city-dwellers to those parks. Unfortunately, the bridges along these parkways were too low for busses to pass beneath them, so public transport couldn't really access his parks. Thus, only wealthy people with parks could visit his parks. Coincidence? No, conscious design on Moses' part! He just didn't want poor people in his parks. 

A potential example of this today, as per Prof. Joyner's lecture: net neutrality, and its abandonment by the FCC.

### Positive change by design

Facebook encourages support for posts throught the Like button. Even when it changed to include 5 other emotions, they didn't include an ability to express dislike for something — just to be angry when hearing something, or sad. This interface seems like it was built in order to encourage positive social interactions online.

Additionally, Facebook encourages change through small tweaks: whereas before it had a limited number of relationship options, it added about 10 new ones (e.g., in a civil union, in a domestic partnership, etc.); same with custom gendering on the platform. 

### Quiz: how do we encourage a white collar worker to move around without explicitly telling them to do so?

- maybe create a crowdsourced weather app that gets people to physically check the weather every hour, so that they're forced to stand up and move?

### Positive change by happenstance

Before the bicycle, women wer reliant on men for transportation. If a woman wanted to go somewhere, she had to go with her spouse or with her father. After the bicycle was invented, women could afford transport themselves wherever they wanted to go, and thus, huge societal changes (e.g., attire for bicycles challenged the traditional fashion standards; huge push for women's independence). In fact, Susan B Anthony once noted that few things have done more for women's liberation than the bicycyle.

### Negative change by happenstance 

Let's take the internet as an example. When the internet emerged, it relied on extant phone lines. Then, it began to rely on cable TV lines. Now, it relies on expensive fiberoptic cables. 

Throughout its history, areas with better infrastructure — i.e., wealthier areas — have had the infrastructure for fast internet first. Even today, there are some poor areas in the US that have satellite services and data caps. 

This isn't purposeful — it's just a natural outgrowth of what makes most economic sense. If we're not careful, completely innocent design ideas can perpetuate negatigve phenomena in society, like social stratification.


### Value-sensitive design

If our interfaces will integrate into people's lives, they need to share the same values of those individuals. The value-sensitive design lab at the University of Washington, for example, seeks to provide theory and method to account for human values in a principled and systematic manner throughout the design process. 

One of the best-developed applications of this design approach is privacy. Privacy by design aims to preserve the values of privacy in systems.

### Paper spotlight: Value sensitive design and information systems - Batya Friedman

This paper covers 3 investigations for approaching value-sensitive design:

1. Conceptual investigations: these are thought experiments that values in play in questions like 
    - who are the direct/indirect stakeholders affected by the design?
    - how are both classes of stakeholders affected?

2. Empirical investigations: these use real users and explore  
    - how they apprehend individual values in the interactive context
    - how do they prioritize individual values and usability considerations?

3. Technical investigations: like empirical investigations, but target systems instead of users. They target whether the systems are compatible with the values of the users.

This paper also outlines some of the features of value-sensitive design:

1. Proactive
2. Distinguishes between usability and human values

### Value-sensitive design across cultures

Different countries and cultures have different values. e.g., the right to be forgotten being a law in the European Union, but not within the US. One culture may run into another culture's value of free speech.

### 5 tips for incorporating value-sensitive design into our interfaces:

1. Start early - identify the values you want to identify early in the process and check on them often! These will likely not only impact the interface, but the core of the task you're trying to support.

2. Know your users - in order to design with values in mind, you need to know your users' values. e.g., Privacy, as a value, is in conflict to some degree, with the value of record keeping.

3. Consider both direct and indirect stakeholders: we often think of direct stakeholders, but indirect ones – those who are affected by the system even thought they don't use it — should also be considered. e.g., a bank system is only used by the bank's employees, but its customers will likely be affected by the design too.

4. Brainstorm the interface's possibilities: think not only of how you're designing the system to be used, but of how it COULD be used (e.g., a system that lets employees log their work hours may be used by their employers as unjust cause for termination). 

5. Choose carefully between supporting and prescribing values: we don't always WANT to change values, but others, like gender equality or economic justice, we do. Be thoughtful about this!

### Reversing the relationship

GE invented a lightbulb that was much more energy-efficient, but scrapped it because it was worried that business needs wouldn't be served by it (i.e., people would use fewer lightbuls, so sales would drop). Similarly, because the web of licenses and interests is so complex, we don't have a single platform to watch platforms like YouTube, HBO, Netflix, and others on TV, which is inconvenient. Technology changes society, but society changes technology too.

### Conclusions

We've discussed the ways in which interfaces interact with existing power structures; how they've impacted society, whether itentionally or unintentionally, and how it interacts with values.

### Lecture 2.10 - Conclusion to principles

We've talked about the various design principles that have emerged from HCI, and in this lesson, we'll tie some of these disaparate threads together.

### Zooming out: human as processor

At the narrowest level, we can see HCI as the interaction between the user and an interface (i.e., human processor model). If we're going to take this model, we have to remember what a human can sense, remember, and physically do. It's almost as if we approach this as an interaction between two computers — like a human and a computer, but a human's actions are thought of computationally. 

The GOMS model approaches HCI in this manner. It distills interaction into Goals, Operators, Methods, and Selection rules, which can be externalized.

### Zooming Out: Human as predictor

For the most part, we're interested in something more task-focused. This is the user interacting through some interface to accomplish some task. This is the predictor model: the user is actively involved in a task, making predictions about what to do, and makign predictions about what will happen. Here's where we looks at the gulfs of execution and evaluation. 

Interfaces, here, ideally disappear from the interaction. Many of our design principles are constructed specifically to help with this: help users make sense of the interface and deal with the underlying tasks. In order to design this interaction effectively, we have to understand how they think about the tasks they're performing; we have to consider their mental models and ensure they match the actual tasks.

Here's where we get into user errors, and issues like expert blind spots and learned helplessness. We have a tool to help us with this: cognitive task analysis, and hierarchical task analysis. 

### Zooming out: human as participant

We're also interested in how this interaction occurs beyond the individual/interface/task: the user isn't just interacting with the task through an interace, but with society as a whole. They're active participants in the world around them. Sometimes we're not just interested in the task that a user's performing, but also in their *motivations and reasons* for performing it. 

That's what activity theory advocates: treating the unit of analysis not as a task, but as an activity, including some of elements of the context surrounding a task. 

Other times, we're interested in how artifacts and minds combine to help accomplish the task – that's what distributed cognition advocates. 

Other times, we're interested in deeply understanding the situated context in which a person is acting — that's where situated action comes in. 

Other times, we're interested in how this integrates with social norms and relationships. That's what social cognition tries to examine. 

At other times, we're interested in going even broader: we're interested in how the interfaces we design create positive social change, or how the interfaces we design may risk perpetuating negative phenomena in society. That's the goal of our design principles: to give an equal world to all people.

### 5 tips: On-screen UI design

1. Use a grid: grids are powerful ways to guide user's sight around the content. There's a reason why newspapers have been using them for decades!

2. Use whitespace: users are good at consuming small chunks of information at a time. News articles, for example, use paragraphs, and highway signs use lots of whitespace.

3. Know your gestalt principles: they refer to how users perceive groups of objects: are they similar? Are they moving together?

4. Reduce clutter: tips 1-3 help with this. 

5. Design in grayscale: color can be a powerful tool, but can run awry of good principles of design. 

### Only half of the picture

Everything we've talked about is only half of the picture. The principles give us a foundation, but to design usable interfaces, we need to design for our user: needfind, prototype, research, and evaluate.

## Lecture 3.7

### Intro to agile methods

Internet ushered in new workflows of HCI. Many developers, for example, use an agile workflow which emphasizes earlier delivery, more continuous improvement, and rapid feedback cycles. This is great — we love feedback cycles!

We'll talk, in this lesson, about how we can use agile development to have faster feedback cycles.

### Demand of Rapid HCI

Where did these changes come from?

Before the age of the internet, *developing* software was expensive. 

*Distribution* of software was also expensive and cumbersome — you had to go to the store and buy it physically. If you fixed software that was hard to use, you had to mail an update disk to each individual user.

The only way to get *feedback* was to have users come in before distribution commenced.

Essentially, you had to get it all right the first time. If you didn't, it'd be difficult to fix.

Now, however, *development* is much cheaper — a person can do what a team used to, in 6 months, in a single day. 

*Distribution* is, essentially, free, and the fixes can be applied in the background.

*Feedback* gathering can automatically be gathered in terms of usage data, and user reports and reviews are widely available. 

Thus, there's much more of an incentive to build something fast and get feedback from users as quickly as possible.

So how do we apply HCI to a rapid, agile develpment process?

### QUIZ

Good candidates for an agile design process are applications that:

a. already use existing devices
b. don't have high stakes associated with them


### When to go agile?

Boehm and Turner suggest that there are several criteria for applying agile development:

1. an environment with low criticality: by its nature, agile development puts some of the testing burden on the user. You don't want bugs or design issues imapcting critical tasks. For smartphone games and social media apps, criticality is pretty low. 

2. It should be a place where requirements change often: the agile process lets you adjust to user needs very quickly. Udacity, for example, is constantly adjusting to novel user needs, so agile development is a good option. 

3. Team size is small

4. Teams are comfortable with change

### Paper spotlight: Towards a framework for integrating agile devleopment and user-centered design

Agile development and user-centered design both emphasize iterative development processes, building on feedback from previous rounds. 

Both also place heavy emphasis on the *user's role* in the development process, and the importance of *team coherence*.

That is to say, they agree on the key element: the importance of the user. 

Conflicts, however, are light. User-centered design philosophies disagree with agile development with regards to:

- the importance of documentation
- the importance of doing research prior to design beginning

Consequently, authors suggest 5 principles for merging user-centered design and agile development:

1. User involvement is high
2. Close team collaboration
3. Prototyping
4. Project lifecycle
(3 and 4 mean that designers should run a sprint ahead of the developers to perform the research necessary for UCD)
5. Strong project management is necessary due to this.

### Live prototyping

We've gotten to the point where constructing working interfaces is just as easy as creating working prototypes. The interface is similar to the tools we use to wireframe — so why bother making prototypes if we can create a final interface just as easily?

Of course, the main reason why we prototype are still applicable:
_we want to get feedback before we roll a bad design out to a larger group_.

Still, if we have a lot of experience making interfaces already, or if we are just making tweaks, live prototyping is a good place to start. This is especially true if:

1. the cost of failure is low
2. the benefit of success is especially high

This is the case with, say, any e-commerce site. 

### A/B testing

Prototypes also allow us to gather feedback from users, and feedback is crucial. 

A/B testing is the name given to rapidly software testing the differences between 2 alternative versions of something. It's the same as t-tests, and allows us to rapidly test software changes with real users. 

We do this by rolling out the new, B, version to a small subset of users, and ensuring that nothing goes terribly wrong. Feedback is received instantly, as the users are using the interface, by seeing the user behavior.

### Agile HCI in design lifecycle

Agile development techniques don't replace the design lifecycle, they just boost it a little. 

We're still doing needfinding, just a bit more tacitly by reading user feedback or checking out interaction logs. We're still brainstorming design alternatives, but often leave them in our head, since we immediately move them to the prototype stage. And our prototypes are still prototypes — they just work. We then quickly evaluate, by only rolling out the new changes that we've made to only a small group of users, to make sure the response is positive. The results of that evaluation feed the same process over and over. 

Having said that, it's still v. important to have an initial needfinding phase that takes longer, since that will put us on the right track towards rapid iteration — one that is more agile.

### 5 tips for mitigating risk in HCI and agile development 

1. start more traditional: start with a more traditional needfinding process and shift to agile development when you'er up and running.

2. Focus on small changes

3. Adopt a parrallel track method. Agile development often uses short, 2-week sprints. Under that setup, have the HCI research be one sprint ahead of the implementation.

4. Be careful with consistency: one of our design principles is consistency, both within our interface and across interface design as a whole. If your interface caters to frequenty visitors/users, you want to be more conservative with how often you mess with their expectations. If you're dealing with one-time users, such as those of museum kiosks, you can be a bit more liberal with the frequency of your changes.

5. Nest your design cycles: in agile development, each cycle gives you a tiny bit of new information. Take this information, and use it in the context of a more traditional design cycle aimed at substantive improvements (instead of small optimizations, as agile development does.)


### Conclusion

Both HCI design and agile development emphasize:

- feedback cycles
- user feedback
- rapid changes

While HCI has done these behind the scenes before reaching real users, agile development does this live. 